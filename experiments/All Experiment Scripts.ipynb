{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Experiment Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import path\n",
    "sys.path.insert(0, '../src')\n",
    "import one_to_n\n",
    "sys.path.insert(0, '../tests')\n",
    "import create_synthetic_dataset\n",
    "import datetime\n",
    "import textdistance\n",
    "import editdistance\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "import editdistance\n",
    "from Matching import core2\n",
    "from Matching import analyze\n",
    "from Matching import matcher\n",
    "import sys\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converts dictionary output to dataframe. Use for experiment results\n",
    "\"\"\"\n",
    "def dict_to_df(output_dict):\n",
    "    df = pd.DataFrame.from_dict(output_dict, orient='index',columns=['Matched Item', 'Max/Min', 'Value'])\n",
    "    return df\n",
    "\"\"\"\n",
    "Converts experiment results to CSV format. Use for experiment results\n",
    "\"\"\"\n",
    "def exp_result_to_csv(filename, experiment_output_df):\n",
    "    #create the CSV with custom filename describing the experiment\n",
    "    df.to_csv(filename, index = False)\n",
    "\n",
    "def create_synth_data(table1_rowcount, table2_rowcount, datafilename1, datafilename2, filename1_dup):\n",
    "    table_a_non_duplicated = create_synthetic_dataset.create_first_df(table1_rowcount)\n",
    "\n",
    "    table_b_non_typo = create_synthetic_dataset.create_second_df(table2_rowcount)\n",
    "\n",
    "    table_b = create_synthetic_dataset.add_typo(table_a_non_duplicated, table_b_non_typo)\n",
    "    \n",
    "    table_a_non_duplicated.to_csv(datafilename1, index = False, header=True)\n",
    "    \n",
    "    table_b.to_csv(datafilename2, index = False, header=True)\n",
    "    \n",
    "    table_a_dup = one_to_n.create_duplicates(table_a_non_duplicated, \"name\", 3)\n",
    "    \n",
    "    table_a_dup.to_csv(filename1_dup, index = False, header=True)\n",
    "    \n",
    "    return table_a_non_duplicated, table_b, table_a_dup\n",
    "\n",
    "def SUM_edit_edge_weight(bip_graph):\n",
    "    for u,v,d in bip_graph.edges(data=True):\n",
    "        val_tuple_1 = u.split(\"_\")\n",
    "        val_tuple_2 = v.split(\"_\")\n",
    "        \n",
    "        if len(val_tuple_1) == 4:\n",
    "            val1 = re.sub(\"[^0-9]\", \"\", val_tuple_1[2])\n",
    "        else: \n",
    "            val1 = re.sub(\"[^0-9]\", \"\", val_tuple_1[1])\n",
    "            \n",
    "        if len(val_tuple_2) == 4:\n",
    "            val2 =re.sub(\"[^0-9]\", \"\", val_tuple_2[2])\n",
    "        else:\n",
    "            val2 =re.sub(\"[^0-9]\", \"\", val_tuple_2[1])\n",
    "\n",
    "        d['weight'] = float(val1) + float(val2)\n",
    "\n",
    "    return bip_graph\n",
    "\n",
    "def minimal_matching(sum_weighted_graph):\n",
    "    new_graph = sum_weighted_graph.copy()\n",
    "    max_weight = max([d['weight'] for u,v,d in new_graph.edges(data=True)])\n",
    "    for u,v,d in new_graph.edges(data=True):\n",
    "        d['weight'] = max_weight - d['weight']\n",
    "\n",
    "    matching_set_minimal = nx.algorithms.matching.max_weight_matching(new_graph)\n",
    "    return matching_set_minimal\n",
    "\n",
    "def fetch_sum(bip_graph, matching):\n",
    "    output = []\n",
    "    for u,v,d in bip_graph.edges(data=True):\n",
    "        l = (u, v)\n",
    "        k = (v, u)\n",
    "        if l in matching:\n",
    "            output.append([u,v, d['weight']])\n",
    "        if k in matching:\n",
    "            output.append([v,u, d['weight']])\n",
    "    return output\n",
    "\n",
    "def formatted_output(out_max, out_min):\n",
    "    out_dict = {}\n",
    "    for (val1,val2, weight) in out_min:\n",
    "        splitted1 = val1.split(\"_\")\n",
    "        splitted2 = val2.split(\"_\")\n",
    "        if len(splitted1) == 4:\n",
    "            if splitted1[0] in out_dict:\n",
    "                out_dict[splitted1[0]].append((splitted2[0], \"min\", weight))\n",
    "            else:\n",
    "                out_dict[splitted1[0]] = [(splitted2[0], \"min\", weight)]\n",
    "\n",
    "        if len(splitted2) == 4:\n",
    "            if splitted2[0] in out_dict:\n",
    "                out_dict[splitted2[0]].append((splitted1[0], \"min\", weight))\n",
    "            else:\n",
    "                out_dict[splitted2[0]] = [(splitted1[0], \"min\", weight)]\n",
    "            \n",
    "    for (val1,val2, weight) in out_max:\n",
    "        splitted1 = val1.split(\"_\")\n",
    "        splitted2 = val2.split(\"_\")\n",
    "        if len(splitted1) == 4:\n",
    "            if splitted1[0] in out_dict:\n",
    "                out_dict[splitted1[0]].append((splitted2[0], \"max\", weight))\n",
    "            else:\n",
    "                out_dict[splitted1[0]] = [(splitted2[0], \"max\", weight)]\n",
    "\n",
    "        if len(splitted2) == 4:\n",
    "            if splitted2[0] in out_dict:\n",
    "                out_dict[splitted2[0]].append((splitted1[0], \"max\", weight))\n",
    "            else:\n",
    "                out_dict[splitted2[0]] = [(splitted1[0], \"max\", weight)]\n",
    "    return out_dict\n",
    "\n",
    "def sum_total_weights(max_min_list):\n",
    "    if max_min_list == [] or max_min_list == None:\n",
    "        print(\"ERROR: NO SIMILARITY FOUND IN NAIVE OR RANDOM SAMPLING APPROACH. Suggestion: Decrease Similarity Matching Threshold.\")\n",
    "        return None\n",
    "    total = 0\n",
    "    for i in max_min_list:\n",
    "        total += i[-1]\n",
    "    return total\n",
    "\n",
    "def sum_bip_script(table_a_non_duplicated, table_b, column_name, similarity_threshold, n_matches):\n",
    "    now = datetime.datetime.now()\n",
    "    bipartite_graph_result = one_to_n.keycomp_treshold_updated_maximal_construct_graph(table_a_non_duplicated, table_b, column_name, similarity_threshold, n_matches)\n",
    "    timing_tresh = (datetime.datetime.now()-now).total_seconds()\n",
    "    print(\"---- Timing for Graph Construction with Treshold Constraint ----\")\n",
    "    print(timing_tresh,\"seconds\")\n",
    "    \n",
    "    sum_weighted_graph = SUM_edit_edge_weight(bipartite_graph_result)\n",
    "    \n",
    "    print(\"\\n\\n 'SUM' MAXIMAL MATCHING:\")\n",
    "    now = datetime.datetime.now()\n",
    "    matching_set_maximal = nx.algorithms.matching.max_weight_matching(sum_weighted_graph)\n",
    "    timing_match = (datetime.datetime.now()-now).total_seconds()\n",
    "#    print(\"The Maximal Matching Set is:\", matching_set_maximal, \"\\n\")\n",
    "    print(\"---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\")\n",
    "    print(timing_match,\"seconds\")\n",
    "    print(\"The number of edges in the graph is:\", sum_weighted_graph.number_of_edges(), \"\\n\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n\\n 'SUM' MINIMAL MATCHING RESULTS:\")\n",
    "    print(nx.bipartite.is_bipartite(sum_weighted_graph))\n",
    "    now = datetime.datetime.now()\n",
    "    matching_set_minimal = minimal_matching(sum_weighted_graph)\n",
    "    timing_match = (datetime.datetime.now()-now).total_seconds()\n",
    "#    print(\"The Minimal Matching Set is:\", matching_set_minimal, \"\\n\")\n",
    "    print(\"---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\")\n",
    "    print(timing_match,\"seconds\")\n",
    "    print(\"The number of edges in the graph is:\", sum_weighted_graph.number_of_edges(), \"\\n\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    out_max = fetch_sum(sum_weighted_graph, matching_set_maximal)\n",
    "    out_min = fetch_sum(sum_weighted_graph, matching_set_minimal)\n",
    "    \n",
    "    form_output = formatted_output(out_max,out_min)\n",
    "    \n",
    "#     max_df_output = dict_to_df(form_output)\n",
    "    \n",
    "#     min_df_output = dict_to_df(form_output)\n",
    "    \n",
    "#     exp_result_to_csv(filename_max_bip_out, max_df_output)\n",
    "    \n",
    "#     exp_result_to_csv(filename_min_bip_out, min_df_output)\n",
    "    \n",
    "    total_max = sum_total_weights(out_max)\n",
    "    print(\"BP Matching: Highest bound for maximum:\", total_max)\n",
    "\n",
    "    total_min = sum_total_weights(out_min)\n",
    "    print(\"BP Matching: Lowest bound for minimum:\", total_min)\n",
    "    \n",
    "    return total_max, total_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_naive_script(n_matches, sim_threshold, filename1_dup, filename2):\n",
    "    # print(os.getcwd())\n",
    "#    cat_table1_dup = core2.data_catalog('s_data1_dup.csv')\n",
    "    cat_table1_dup = core2.data_catalog(filename1_dup)\n",
    "    cat_table2 = core2.data_catalog(filename2)\n",
    "    print('Loaded catalogs.')\n",
    "    \n",
    "    \n",
    "    # NAIVE MAX MATCHING\n",
    "    print(\"NAIVE MAX MATCHING\")\n",
    "    print('Performing compare all match (edit distance)...')\n",
    "    now = datetime.datetime.now()\n",
    "    max_compare_all_edit_match = matcher.matcher_dup_updated(n_matches, cat_table1_dup,cat_table2,editdistance.eval, matcher.all, sim_threshold)\n",
    "    naive_time_edit = (datetime.datetime.now()-now).total_seconds()\n",
    "    print(\"Naive Edit Distance Matching computation time taken: \", naive_time_edit, \" seconds\")\n",
    "    #print('Compare All Matcher (Edit Distance) Performance: ' + str(core.eval_matching(compare_all_edit_match)))\n",
    "\n",
    "\n",
    "#     print('Performing compare all match (jaccard distance)...')\n",
    "#     now = datetime.datetime.now()\n",
    "#     max_compare_all_jaccard_match = matcher.matcher_dup_updated(n_matches, cat_table1_dup,cat_table2,analyze.jaccard_calc, matcher.all, sim_threshold)\n",
    "#     naive_time_jaccard = (datetime.datetime.now()-now).total_seconds()\n",
    "#     print(\"Naive Jaccard Matching computation time taken: \", naive_time_jaccard, \" seconds\", \"\\n\")\n",
    "    #print('Compare All Matcher (Jaccard Distance) Performance: ' + str(core2.eval_matching(compare_all_jaccard_match)))\n",
    "\n",
    "    # NAIVE MIN MATCHING\n",
    "    print(\"NAIVE MIN MATCHING\")\n",
    "    print('Performing compare all match (edit distance)...')\n",
    "    now = datetime.datetime.now()\n",
    "    min_compare_all_edit_match = matcher.matcher_updated(n_matches, cat_table1_dup,cat_table2,editdistance.eval, matcher.all, sim_threshold)\n",
    "    naive_time_edit = (datetime.datetime.now()-now).total_seconds()\n",
    "    print(\"Naive Edit Distance Matching computation time taken: \", naive_time_edit, \" seconds\")\n",
    "    #print('Compare All Matcher (Edit Distance) Performance: ' + str(core.eval_matching(compare_all_edit_match)))\n",
    "\n",
    "\n",
    "#     print('Performing compare all match (jaccard distance)...')\n",
    "#     now = datetime.datetime.now()\n",
    "#     min_compare_all_jaccard_match = matcher.matcher_updated(n_matches, cat_table1,cat_table2,analyze.jaccard_calc, matcher.all, sim_threshold)\n",
    "#     naive_time_jaccard = (datetime.datetime.now()-now).total_seconds()\n",
    "#     print(\"Naive Jaccard Matching computation time taken: \", naive_time_jaccard, \" seconds\")\n",
    "    #print('Compare All Matcher (Jaccard Distance) Performance: ' + str(core2.eval_matching(compare_all_jaccard_match)))\n",
    "\n",
    "    naive_total_max = sum_total_weights(max_compare_all_edit_match)\n",
    "    naive_total_min = sum_total_weights(min_compare_all_edit_match)\n",
    "    print(\"MAX Matching Bound:\")\n",
    "    print(\"NAIVE Matching: \", naive_total_max)\n",
    "    print(\"MIN Matching Bound:\")\n",
    "    print(\"NAIVE Matching: \", naive_total_min)\n",
    "    return naive_total_max, naive_total_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_random_sample_script(n_matches, sim_threshold, sample_size, filename1_dup, filename2):\n",
    "    \n",
    "    cat_table1_dup = core2.data_catalog(filename1_dup)\n",
    "    cat_table2 = core2.data_catalog(filename2)\n",
    "    print('Loaded catalogs.')\n",
    "    \n",
    "    # RANDOM SAMPLING MAX MATCHING\n",
    "    print(\"RANDOM SAMPLE MAX MATCHING\")\n",
    "    print('Performing random sample match (edit distance)...')\n",
    "    now = datetime.datetime.now()\n",
    "    max_compare_sampled_edit_match = matcher.matcher_dup_updated(n_matches, cat_table1_dup,cat_table2,editdistance.eval, matcher.random_sample, sim_threshold, sample_size)\n",
    "    sim_time_edit = (datetime.datetime.now()-now).total_seconds()\n",
    "    print(\"Simulation-Based Edit Distance Matching computation time taken: \", sim_time_edit, \" seconds\")\n",
    "    #print('Random Sample Matcher (Edit Distance) Performance: ' + str(core.eval_matching(compare_all_edit_match)))\n",
    "\n",
    "    # print('Performing random sample match (jaccard distance)...')\n",
    "    # now = datetime.datetime.now()\n",
    "    # max_compare_sampled_jaccard_match = matcher.matcher_dup_updated(cat_table1_dup,cat_table2,analyze.jaccard_calc, matcher.random_sample, sim_threshold, sample_size)\n",
    "    # sim_time_jaccard = (datetime.datetime.now()-now).total_seconds()\n",
    "    # print(\"Simulation-Based Jaccard Matching computation time taken: \", sim_time_jaccard, \" seconds\", \"\\n\")\n",
    "    #print('Random Sample Matcher (Jaccard Distance) Performance: ' + str(core.eval_matching(compare_all_jaccard_match)))\n",
    "\n",
    "\n",
    "    # RANDOM SAMPLING MIN MATCHING\n",
    "    print(\"RANDOM SAMPLE MIN MATCHING\")\n",
    "    print('Performing random sample match (edit distance)...')\n",
    "    now = datetime.datetime.now()\n",
    "    min_compare_sampled_edit_match = matcher.matcher_updated(n_matches, cat_table1_dup,cat_table2,editdistance.eval, matcher.random_sample, sim_threshold, sample_size)\n",
    "    sim_time_edit = (datetime.datetime.now()-now).total_seconds()\n",
    "    print(\"Simulation-Based Edit Distance Matching computation time taken: \", sim_time_edit, \" seconds\")\n",
    "    #print('Random Sample Matcher (Edit Distance) Performance: ' + str(core.eval_matching(compare_all_edit_match)))\n",
    "\n",
    "    # print('Performing random sample match (jaccard distance)...')\n",
    "    # now = datetime.datetime.now()\n",
    "    # min_compare_sampled_jaccard_match = matcher.matcher_updated(cat_table1,cat_table2,analyze.jaccard_calc, matcher.random_sample, sim_threshold, sample_size)\n",
    "    # sim_time_jaccard = (datetime.datetime.now()-now).total_seconds()\n",
    "    # print(\"Simulation-Based Jaccard Matching computation time taken: \", sim_time_jaccard, \" seconds\")\n",
    "    #print('Random Sample Matcher (Jaccard Distance) Performance: ' + str(core.eval_matching(compare_all_jaccard_match)))\n",
    "\n",
    "    sampled_total_max = sum_total_weights(max_compare_sampled_edit_match)\n",
    "    sampled_total_min = sum_total_weights(min_compare_sampled_edit_match)\n",
    "    print(\"MAX Matching Bound:\")\n",
    "    print(\"SAMPLED Matching: \", sampled_total_max, \"\\n\")\n",
    "    print(\"MIN Matching Bound:\")\n",
    "    print(\"NAIVE Matching: \", naive_total_min)\n",
    "    return sampled_total_max, sampled_total_min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_results_summaries(bip_max, bip_min, naive_max, naive_min, random_max, random_min):\n",
    "    print(\"MAX Matching Bound:\")\n",
    "    print(\"BP Matching: \", bip_sum_max)\n",
    "    print(\"NAIVE Matching: \", naive_total_max)\n",
    "    print(\"SAMPLED Matching: \", sampled_total_max, \"\\n\")\n",
    "\n",
    "    naive_total_min = sum_total_weights(min_compare_all_edit_match)\n",
    "    sampled_total_min = sum_total_weights(min_compare_sampled_edit_match)\n",
    "    bp_total_min = sum_total_weights(out_min)\n",
    "\n",
    "    print(\"MIN Matching Bound:\")\n",
    "    print(\"BP Matching: \", bip_sum_min)\n",
    "    print(\"NAIVE Matching: \", naive_total_min)\n",
    "    print(\"SAMPLED Matching: \", sampled_total_min, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SUM:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Balanced 2 tables matching, n=3\n",
    "#### Matching [Total Max, Total Min] Outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Configuration Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_a_non_duplicated, table_b, table_a_dup = create_synth_data(100, 100, \"table1\", \"table2\", \"table1_dup\")\n",
    "# print(table_a_non_duplicated, table_b, table_a_dup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bipartite Matching Experiment Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Timing for Graph Construction with Treshold Constraint ----\n",
      "0.227045 seconds\n",
      "\n",
      "\n",
      " 'SUM' MAXIMAL MATCHING:\n",
      "---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\n",
      "0.239413 seconds\n",
      "The number of edges in the graph is: 1545 \n",
      "\n",
      "\n",
      "\n",
      " 'SUM' MINIMAL MATCHING RESULTS:\n",
      "True\n",
      "---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\n",
      "0.249257 seconds\n",
      "The number of edges in the graph is: 1545 \n",
      "\n",
      "BP Matching: Highest bound for maximum: 1086.0\n",
      "BP Matching: Lowest bound for minimum: 631.0\n"
     ]
    }
   ],
   "source": [
    "total_max, total_min = sum_bip_script(table_a_non_duplicated, table_b, \"name\", 0.09, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Matching Experiment Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded catalogs.\n",
      "NAIVE MAX MATCHING\n",
      "Performing compare all match (edit distance)...\n",
      "Naive Edit Distance Matching computation time taken:  0.210387  seconds\n",
      "NAIVE MIN MATCHING\n",
      "Performing compare all match (edit distance)...\n",
      "Naive Edit Distance Matching computation time taken:  0.190092  seconds\n",
      "MAX Matching Bound:\n",
      "NAIVE Matching:  10176\n",
      "MIN Matching Bound:\n",
      "NAIVE Matching:  6669\n"
     ]
    }
   ],
   "source": [
    "naive_total_max, naive_total_min = sum_naive_script(3, 0.09, \"table1_dup\", \"table2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Sampling Matching Experiment Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded catalogs.\n",
      "RANDOM SAMPLE MAX MATCHING\n",
      "Performing random sample match (edit distance)...\n",
      "Simulation-Based Edit Distance Matching computation time taken:  0.135151  seconds\n",
      "RANDOM SAMPLE MIN MATCHING\n",
      "Performing random sample match (edit distance)...\n",
      "Simulation-Based Edit Distance Matching computation time taken:  0.13062  seconds\n",
      "MAX Matching Bound:\n",
      "SAMPLED Matching:  5134 \n",
      "\n",
      "MIN Matching Bound:\n",
      "NAIVE Matching:  6669\n"
     ]
    }
   ],
   "source": [
    "sampled_total_max, sampled_total_min = sum_random_sample_script(3, 0.09, 50, \"table1_dup\", \"table2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching [Total Max, Total Min] Outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bipartite Matching:  [ 631.0 ,  1086.0 ]\n",
      "Naive Matching:  [ 6669 ,  10176 ]\n",
      "Random Sample Matching:  [ 3887 ,  5134 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Bipartite Matching: \", \"[\", total_min, \", \", total_max, \"]\")\n",
    "print(\"Naive Matching: \", \"[\", naive_total_min, \", \", naive_total_max, \"]\")\n",
    "print(\"Random Sample Matching: \", \"[\", sampled_total_min, \", \", sampled_total_max, \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COUNT:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CASE 1: Write Case Here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bipartite matching Experiment Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_bip_script(table_a_non_duplicated, table_b, column_name, similarity_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Matching Experiment Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_naive_script(filename1_dup, filename2):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Sampling Matching Experiment Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_random_sample_script(sample_size, filename1_dup, filename2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: 1's table disproportionately bigger than n's table. \n",
    "i.e. table_a has 10,000 rows, table_b has 100 rows, n=3, sim_threshold = 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Configuration Here\n",
    "table_a_non_duplicated, table_b, table_a_dup = create_synth_data(10000, 100, \"table1\", \"table2\", \"table1_dup\")\n",
    "# print(table_a_non_duplicated, table_b, table_a_dup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0% complete\n",
      "20.0% complete\n",
      "30.0% complete\n",
      "40.0% complete\n",
      "50.0% complete\n",
      "60.0% complete\n",
      "70.0% complete\n",
      "80.0% complete\n",
      "90.0% complete\n",
      "100.0% complete\n",
      "110.0% complete\n",
      "120.0% complete\n",
      "130.0% complete\n",
      "140.0% complete\n",
      "150.0% complete\n",
      "160.0% complete\n",
      "170.0% complete\n",
      "180.0% complete\n",
      "190.0% complete\n",
      "200.0% complete\n",
      "210.0% complete\n",
      "220.0% complete\n",
      "230.0% complete\n",
      "240.0% complete\n",
      "250.0% complete\n",
      "260.0% complete\n",
      "270.0% complete\n",
      "280.0% complete\n",
      "---- Timing for Graph Construction with Treshold Constraint ----\n",
      "21.457723 seconds\n",
      "\n",
      "\n",
      " 'SUM' MAXIMAL MATCHING:\n",
      "---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\n",
      "12.069526 seconds\n",
      "The number of edges in the graph is: 156996 \n",
      "\n",
      "\n",
      "\n",
      " 'SUM' MINIMAL MATCHING RESULTS:\n",
      "True\n",
      "---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\n",
      "11.577488 seconds\n",
      "The number of edges in the graph is: 156996 \n",
      "\n",
      "BP Matching: Highest bound for maximum: 1304.0\n",
      "BP Matching: Lowest bound for minimum: 419.0\n",
      "Loaded catalogs.\n",
      "NAIVE MAX MATCHING\n",
      "Performing compare all match (edit distance)...\n",
      "Naive Edit Distance Matching computation time taken:  15.924901  seconds\n",
      "NAIVE MIN MATCHING\n",
      "Performing compare all match (edit distance)...\n",
      "Naive Edit Distance Matching computation time taken:  15.136646  seconds\n",
      "MAX Matching Bound:\n",
      "NAIVE Matching:  1014006\n",
      "MIN Matching Bound:\n",
      "NAIVE Matching:  444210\n",
      "Loaded catalogs.\n",
      "RANDOM SAMPLE MAX MATCHING\n",
      "Performing random sample match (edit distance)...\n",
      "Simulation-Based Edit Distance Matching computation time taken:  11.074858  seconds\n",
      "RANDOM SAMPLE MIN MATCHING\n",
      "Performing random sample match (edit distance)...\n",
      "Simulation-Based Edit Distance Matching computation time taken:  11.499942  seconds\n",
      "MAX Matching Bound:\n",
      "SAMPLED Matching:  505805 \n",
      "\n",
      "MIN Matching Bound:\n",
      "NAIVE Matching:  444210\n"
     ]
    }
   ],
   "source": [
    "#### Bipartite Matching Experiment Scripts\n",
    "total_max, total_min = sum_bip_script(table_a_non_duplicated, table_b, \"name\", 0.09, 3)\n",
    "#### Naive Matching Experiment Scripts\n",
    "naive_total_max, naive_total_min = sum_naive_script(3, 0.09, \"table1_dup\", \"table2\")\n",
    "#### Random Sampling Matching Experiment Scripts\n",
    "sampled_total_max, sampled_total_min = sum_random_sample_script(3, 0.09, 50, \"table1_dup\", \"table2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bipartite Matching:  [ 419.0 ,  1304.0 ]\n",
      "Naive Matching:  [ 444210 ,  1014006 ]\n",
      "Random Sample Matching:  [ 274261 ,  505805 ]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Observation: Bipartite Matching gave the lowest outcome. Why?\n",
    "\n",
    "\n",
    "Matching [Total Max, Total Min] Outcome\n",
    "\"\"\"\n",
    "print(\"Bipartite Matching: \", \"[\", total_min, \", \", total_max, \"]\")\n",
    "print(\"Naive Matching: \", \"[\", naive_total_min, \", \", naive_total_max, \"]\")\n",
    "print(\"Random Sample Matching: \", \"[\", sampled_total_min, \", \", sampled_total_max, \"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
