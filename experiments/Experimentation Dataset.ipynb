{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments Using Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import path\n",
    "sys.path.insert(0, '../src')\n",
    "import one_to_n\n",
    "sys.path.insert(0, '../tests')\n",
    "import create_synthetic_dataset\n",
    "import datetime\n",
    "import textdistance\n",
    "import editdistance\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bipartite Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 name age\n",
      "0    Lorraine Herrera   0\n",
      "1         Mike Henson   0\n",
      "2         Jeremy Shaw   8\n",
      "3     Amy Christensen   1\n",
      "4       Jeremy Morris   7\n",
      "5           Mark Moon   8\n",
      "6  Elizabeth Valencia   1\n",
      "7   Kimberly Marshall   1\n",
      "8       Patricia Haas   1\n",
      "9       Luis Robinson   7\n",
      "                    name  age\n",
      "0      Lorraine HerreraY    9\n",
      "1     Lorraine HerreraXO    5\n",
      "2         Mike HensonfjO    1\n",
      "3         Jeremy ShawHYN    7\n",
      "4     Amy ChristensenGYi    1\n",
      "5       Jeremy MorrisVTD    4\n",
      "6           Mark MoonVYN    7\n",
      "7  Elizabeth ValenciabAh    0\n",
      "8   Kimberly MarshallFnX    2\n",
      "9       Patricia HaasFyv    5\n",
      "---- Timing for Graph Construction with Treshold Constraint ----\n",
      "0.00945 seconds\n"
     ]
    }
   ],
   "source": [
    "table_a = create_synthetic_dataset.create_first_df(10)\n",
    "\n",
    "table_b_non_typo = create_synthetic_dataset.create_second_df(10)\n",
    "\n",
    "table_b = create_synthetic_dataset.add_typo(table_a, table_b_non_typo)\n",
    "\n",
    "print(table_a)\n",
    "print(table_b)\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "bipartite_graph_result = one_to_n.keycomp_treshold_updated_maximal_construct_graph(table_a, table_b, \"name\", 0.5)\n",
    "timing_tresh = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"---- Timing for Graph Construction with Treshold Constraint ----\")\n",
    "print(timing_tresh,\"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SUM_edit_edge_weight(bip_graph):\n",
    "    for u,v,d in bip_graph.edges(data=True):\n",
    "        val_tuple_1 = u.split(\"_\")\n",
    "        val_tuple_2 = v.split(\"_\")\n",
    "        \n",
    "        if len(val_tuple_1) == 4:\n",
    "            val1 = re.sub(\"[^0-9]\", \"\", val_tuple_1[2])\n",
    "        else: \n",
    "            val1 = re.sub(\"[^0-9]\", \"\", val_tuple_1[1])\n",
    "            \n",
    "        if len(val_tuple_2) == 4:\n",
    "            val2 =re.sub(\"[^0-9]\", \"\", val_tuple_2[2])\n",
    "        else:\n",
    "            val2 =re.sub(\"[^0-9]\", \"\", val_tuple_2[1])\n",
    "\n",
    "        d['weight'] = float(val1) + float(val2)\n",
    "\n",
    "    return bip_graph\n",
    "\n",
    "sum_weighted_graph = SUM_edit_edge_weight(bipartite_graph_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximal Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 'SUM' MAXIMAL MATCHING:\n",
      "The Maximal Matching Set is: {('Mark MoonVYN_(7,)_2', 'Jeremy Shaw_2_(8,)_1'), ('Mark Moon_2_(8,)_1', 'Lorraine HerreraY_(9,)_2'), ('Mark Moon_0_(8,)_1', 'Lorraine HerreraXO_(5,)_2'), ('Patricia HaasFyv_(5,)_2', 'Jeremy Shaw_1_(8,)_1'), ('Jeremy Morris_2_(7,)_1', 'Kimberly MarshallFnX_(2,)_2'), ('Jeremy Shaw_0_(8,)_1', 'Elizabeth ValenciabAh_(0,)_2'), ('Luis Robinson_1_(7,)_1', 'Amy ChristensenGYi_(1,)_2'), ('Jeremy Morris_1_(7,)_1', 'Mike HensonfjO_(1,)_2'), ('Luis Robinson_2_(7,)_1', 'Jeremy MorrisVTD_(4,)_2'), ('Jeremy ShawHYN_(7,)_2', 'Mark Moon_1_(8,)_1')} \n",
      "\n",
      "---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\n",
      "0.013824 seconds\n",
      "The number of edges in the graph is: 300 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n 'SUM' MAXIMAL MATCHING:\")\n",
    "now = datetime.datetime.now()\n",
    "matching_set_maximal = nx.algorithms.matching.max_weight_matching(sum_weighted_graph)\n",
    "timing_match = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"The Maximal Matching Set is:\", matching_set_maximal, \"\\n\")\n",
    "print(\"---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\")\n",
    "print(timing_match,\"seconds\")\n",
    "print(\"The number of edges in the graph is:\", sum_weighted_graph.number_of_edges(), \"\\n\")\n",
    "\n",
    "\n",
    "# print(\"The Maximal Matching Set is:\", matching_set_maximal, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimal Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_matching(sum_weighted_graph):\n",
    "\n",
    "    new_graph = sum_weighted_graph.copy()\n",
    "    max_weight = max([d['weight'] for u,v,d in new_graph.edges(data=True)])\n",
    "    for u,v,d in new_graph.edges(data=True):\n",
    "        d['weight'] = max_weight - d['weight']\n",
    "\n",
    "    matching_set_minimal = nx.algorithms.matching.max_weight_matching(new_graph)\n",
    "    return matching_set_minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " 'SUM' MINIMAL MATCHING RESULTS:\n",
      "True\n",
      "The Minimal Matching Set is: {('Jeremy MorrisVTD_(4,)_2', 'Mike Henson_0_(0,)_1'), ('Jeremy ShawHYN_(7,)_2', 'Elizabeth Valencia_2_(1,)_1'), ('Mike Henson_2_(0,)_1', 'Elizabeth ValenciabAh_(0,)_2'), ('Amy Christensen_0_(1,)_1', 'Lorraine HerreraXO_(5,)_2'), ('Lorraine Herrera_1_(0,)_1', 'Kimberly MarshallFnX_(2,)_2'), ('Mark MoonVYN_(7,)_2', 'Kimberly Marshall_2_(1,)_1'), ('Patricia HaasFyv_(5,)_2', 'Patricia Haas_2_(1,)_1'), ('Amy ChristensenGYi_(1,)_2', 'Lorraine Herrera_2_(0,)_1'), ('Lorraine Herrera_0_(0,)_1', 'Lorraine HerreraY_(9,)_2'), ('Mike Henson_1_(0,)_1', 'Mike HensonfjO_(1,)_2')} \n",
      "\n",
      "---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\n",
      "0.015914 seconds\n",
      "The number of edges in the graph is: 300 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\\n 'SUM' MINIMAL MATCHING RESULTS:\")\n",
    "print(nx.bipartite.is_bipartite(sum_weighted_graph))\n",
    "now = datetime.datetime.now()\n",
    "matching_set_minimal = minimal_matching(sum_weighted_graph)\n",
    "timing_match = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"The Minimal Matching Set is:\", matching_set_minimal, \"\\n\")\n",
    "print(\"---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\")\n",
    "print(timing_match,\"seconds\")\n",
    "print(\"The number of edges in the graph is:\", sum_weighted_graph.number_of_edges(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " MAX MATCHES WITH SUM WEIGHTS\n",
      "['Mark Moon_2_(8,)_1', 'Lorraine HerreraY_(9,)_2', 17.0]\n",
      "['Mark Moon_0_(8,)_1', 'Lorraine HerreraXO_(5,)_2', 13.0]\n",
      "['Jeremy Morris_1_(7,)_1', 'Mike HensonfjO_(1,)_2', 8.0]\n",
      "['Jeremy ShawHYN_(7,)_2', 'Mark Moon_1_(8,)_1', 15.0]\n",
      "['Luis Robinson_1_(7,)_1', 'Amy ChristensenGYi_(1,)_2', 8.0]\n",
      "['Luis Robinson_2_(7,)_1', 'Jeremy MorrisVTD_(4,)_2', 11.0]\n",
      "['Mark MoonVYN_(7,)_2', 'Jeremy Shaw_2_(8,)_1', 15.0]\n",
      "['Jeremy Shaw_0_(8,)_1', 'Elizabeth ValenciabAh_(0,)_2', 8.0]\n",
      "['Jeremy Morris_2_(7,)_1', 'Kimberly MarshallFnX_(2,)_2', 9.0]\n",
      "['Patricia HaasFyv_(5,)_2', 'Jeremy Shaw_1_(8,)_1', 13.0]\n",
      "\n",
      " MIN MATCHES WITH SUM WEIGHTS\n",
      "['Lorraine Herrera_0_(0,)_1', 'Lorraine HerreraY_(9,)_2', 9.0]\n",
      "['Amy Christensen_0_(1,)_1', 'Lorraine HerreraXO_(5,)_2', 6.0]\n",
      "['Mike Henson_1_(0,)_1', 'Mike HensonfjO_(1,)_2', 1.0]\n",
      "['Jeremy ShawHYN_(7,)_2', 'Elizabeth Valencia_2_(1,)_1', 8.0]\n",
      "['Amy ChristensenGYi_(1,)_2', 'Lorraine Herrera_2_(0,)_1', 1.0]\n",
      "['Jeremy MorrisVTD_(4,)_2', 'Mike Henson_0_(0,)_1', 4.0]\n",
      "['Mark MoonVYN_(7,)_2', 'Kimberly Marshall_2_(1,)_1', 8.0]\n",
      "['Mike Henson_2_(0,)_1', 'Elizabeth ValenciabAh_(0,)_2', 0.0]\n",
      "['Lorraine Herrera_1_(0,)_1', 'Kimberly MarshallFnX_(2,)_2', 2.0]\n",
      "['Patricia HaasFyv_(5,)_2', 'Patricia Haas_2_(1,)_1', 6.0]\n",
      "\n",
      " FORMAL OUTPUT\n",
      "Lorraine Herrera [('Lorraine HerreraY', 'min', 9.0), ('Amy ChristensenGYi', 'min', 1.0), ('Kimberly MarshallFnX', 'min', 2.0)]\n",
      "Amy Christensen [('Lorraine HerreraXO', 'min', 6.0)]\n",
      "Mike Henson [('Mike HensonfjO', 'min', 1.0), ('Jeremy MorrisVTD', 'min', 4.0), ('Elizabeth ValenciabAh', 'min', 0.0)]\n",
      "Elizabeth Valencia [('Jeremy ShawHYN', 'min', 8.0)]\n",
      "Kimberly Marshall [('Mark MoonVYN', 'min', 8.0)]\n",
      "Patricia Haas [('Patricia HaasFyv', 'min', 6.0)]\n",
      "Mark Moon [('Lorraine HerreraY', 'max', 17.0), ('Lorraine HerreraXO', 'max', 13.0), ('Jeremy ShawHYN', 'max', 15.0)]\n",
      "Jeremy Morris [('Mike HensonfjO', 'max', 8.0), ('Kimberly MarshallFnX', 'max', 9.0)]\n",
      "Luis Robinson [('Amy ChristensenGYi', 'max', 8.0), ('Jeremy MorrisVTD', 'max', 11.0)]\n",
      "Jeremy Shaw [('Mark MoonVYN', 'max', 15.0), ('Elizabeth ValenciabAh', 'max', 8.0), ('Patricia HaasFyv', 'max', 13.0)]\n"
     ]
    }
   ],
   "source": [
    "def fetch_sum(bip_graph, matching):\n",
    "    output = []\n",
    "    for u,v,d in bip_graph.edges(data=True):\n",
    "        l = (u, v)\n",
    "        k = (v, u)\n",
    "        if l in matching:\n",
    "            output.append([u,v, d['weight']])\n",
    "        if k in matching:\n",
    "            output.append([v,u, d['weight']])\n",
    "    return output\n",
    "\n",
    "out_max = fetch_sum(sum_weighted_graph, matching_set_maximal)\n",
    "out_min = fetch_sum(sum_weighted_graph, matching_set_minimal)\n",
    "\n",
    "print(\"\\n MAX MATCHES WITH SUM WEIGHTS\")\n",
    "for val in out_max:\n",
    "    print(val)\n",
    "    \n",
    "print(\"\\n MIN MATCHES WITH SUM WEIGHTS\")\n",
    "for val2 in out_min:\n",
    "    print(val2)\n",
    "\n",
    "def formatted_output(out_max, out_min):\n",
    "    out_dict = {}\n",
    "    for (val1,val2, weight) in out_min:\n",
    "        splitted1 = val1.split(\"_\")\n",
    "        splitted2 = val2.split(\"_\")\n",
    "        if len(splitted1) == 4:\n",
    "            if splitted1[0] in out_dict:\n",
    "                out_dict[splitted1[0]].append((splitted2[0], \"min\", weight))\n",
    "            else:\n",
    "                out_dict[splitted1[0]] = [(splitted2[0], \"min\", weight)]\n",
    "\n",
    "        if len(splitted2) == 4:\n",
    "            if splitted2[0] in out_dict:\n",
    "                out_dict[splitted2[0]].append((splitted1[0], \"min\", weight))\n",
    "            else:\n",
    "                out_dict[splitted2[0]] = [(splitted1[0], \"min\", weight)]\n",
    "            \n",
    "    for (val1,val2, weight) in out_max:\n",
    "        splitted1 = val1.split(\"_\")\n",
    "        splitted2 = val2.split(\"_\")\n",
    "        if len(splitted1) == 4:\n",
    "            if splitted1[0] in out_dict:\n",
    "                out_dict[splitted1[0]].append((splitted2[0], \"max\", weight))\n",
    "            else:\n",
    "                out_dict[splitted1[0]] = [(splitted2[0], \"max\", weight)]\n",
    "\n",
    "        if len(splitted2) == 4:\n",
    "            if splitted2[0] in out_dict:\n",
    "                out_dict[splitted2[0]].append((splitted1[0], \"max\", weight))\n",
    "            else:\n",
    "                out_dict[splitted2[0]] = [(splitted1[0], \"max\", weight)]\n",
    "    return out_dict\n",
    "            \n",
    "    \n",
    "form_output = formatted_output(out_max,out_min)\n",
    "\n",
    "print(\"\\n FORMAL OUTPUT\")\n",
    "for i,val in form_output.items():\n",
    "    print(i, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Naive Matching Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "\n",
    "print('Loaded catalogs.')\n",
    "print('Performing compare all match (edit distance)...')\n",
    "now = datetime.datetime.now()\n",
    "compare_all_edit_match = matcher.matcher(amzn,goog,editdistance.eval, matcher.all)\n",
    "naive_time_edit = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"Naive Edit Distance Matching computation time taken: \", naive_time_edit, \" seconds\")\n",
    "print('Compare All Matcher (Edit Distance) Performance: ' + str(core.eval_matching(compare_all_edit_match)))\n",
    "\n",
    "\n",
    "\n",
    "print('Performing compare all match (jaccard distance)...')\n",
    "now = datetime.datetime.now()\n",
    "compare_all_jaccard_match = matcher.matcher(amzn,goog,analyze.jaccard_calc, matcher.all)\n",
    "naive_time_jaccard = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"Naive Jaccard Matching computation time taken: \", naive_time_jaccard, \" seconds\")\n",
    "print('Compare All Matcher (Jaccard Distance) Performance: ' + str(core.eval_matching(compare_all_jaccard_match)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Random Sampling Matching Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Performing random sample match (edit distance)...')\n",
    "now = datetime.datetime.now()\n",
    "compare_all_edit_match = matcher.matcher(amzn,goog,editdistance.eval, matcher.random_sample, sample_size)\n",
    "sim_time_edit = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"Simulation-Based Edit Distance Matching computation time taken: \", sim_time_edit, \" seconds\")\n",
    "print('Random Sample Matcher (Edit Distance) Performance: ' + str(core.eval_matching(compare_all_edit_match)))\n",
    "\n",
    "print('Performing random sample match (jaccard distance)...')\n",
    "now = datetime.datetime.now()\n",
    "compare_all_jaccard_match = matcher.matcher(amzn,goog,analyze.jaccard_calc, matcher.random_sample, sample_size)\n",
    "sim_time_jaccard = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"Simulation-Based Jaccard Matching computation time taken: \", sim_time_jaccard, \" seconds\")\n",
    "print('Random Sample Matcher (Jaccard Distance) Performance: ' + str(core.eval_matching(compare_all_jaccard_match)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
