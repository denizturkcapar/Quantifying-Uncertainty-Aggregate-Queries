{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximal and Minimal Bipartite Matching for Inexact Matches Between 2 Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goal: Explore some interesting problems around using PC's to model fuzzy joins \n",
    "\n",
    "#### Some examples: inexact matches between tables, finding the highest and lowest possible aggregate after a match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Build a maximal bipartite matching algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define a bipartite graph. (Assume we are given any similarity metric - this similarity metric is taken from geeksforgeeks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editDistance(str1, str2, m, n): \n",
    "  \n",
    "    # If first string is empty, the only option is to \n",
    "    # insert all characters of second string into first \n",
    "    if m == 0: \n",
    "         return n \n",
    "  \n",
    "    # If second string is empty, the only option is to \n",
    "    # remove all characters of first string \n",
    "    if n == 0: \n",
    "        return m \n",
    "  \n",
    "    # If last characters of two strings are same, nothing \n",
    "    # much to do. Ignore last characters and get count for \n",
    "    # remaining strings. \n",
    "    if str1[m-1]== str2[n-1]: \n",
    "        return editDistance(str1, str2, m-1, n-1) \n",
    "  \n",
    "    # If last characters are not same, consider all three \n",
    "    # operations on last character of first string, recursively \n",
    "    # compute minimum cost for all three operations and take \n",
    "    # minimum of three values. \n",
    "    return 1 + min(editDistance(str1, str2, m, n-1),    # Insert \n",
    "                   editDistance(str1, str2, m-1, n),    # Remove \n",
    "                   editDistance(str1, str2, m-1, n-1)    # Replace \n",
    "                   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def similarity(x,y):\n",
    "    return editDistance(x,y,len(x),len(y))\n",
    "\n",
    "def construct_graph(table_a, table_b):\n",
    "    bipartite_graph = nx.Graph()\n",
    "    for first in table_a:\n",
    "        key1, val1 = first\n",
    "        id1 = key1 + '_' + str(val1) + '_1'\n",
    "        for second in table_b:\n",
    "            key2, val2 = second\n",
    "            id2 = key2 + '_' + str(val2) + '_2' #add value to identifier to disitnguish two entries with different values\n",
    "            bipartite_graph.add_edge(id1, id2, weight=1/(1+similarity(key1,key2))) #edit distance and weight should be inv. prop.\n",
    "                                                                                    #also adding 1 to denom. to prevent divide by 0\n",
    "            # add 1,2 to distinguish two key-value tuples belonging to different tables\n",
    "    return bipartite_graph\n",
    "\n",
    "# import dictionary for graph \n",
    "from collections import defaultdict \n",
    "\n",
    "# defaultdict allows that if a key is not found in the dictionary, \n",
    "# then instead of a KeyError being thrown, a new entry is created.\n",
    "table_a = [('US', '300 M'), ('CN', '300 M'), ('CA', '300 M'), ('AU', '300 M'),('USA', '35 T')] \n",
    "table_b = [('USA', '35 T'), ('USA', '32 T'), ('UK', '3 T'), ('AUS', '20 T'), ('CAL', '22 T')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Table A:\n",
    "\n",
    "US -> 300 M\n",
    "\n",
    "CN -> 12 B\n",
    "\n",
    "CA -> 50 M\n",
    "\n",
    "AU -> 25 M\n",
    "\n",
    "#### Table B\n",
    "\n",
    "USA -> 35 T\n",
    "\n",
    "UK -> 3 T\n",
    "\n",
    "USA -> 32 T\n",
    "\n",
    "AUS -> 20 T\n",
    "\n",
    "CAL -> 22 T\n",
    "\n",
    "table_a = { \"US\" : [\"300 M\"],\n",
    "          \"CN\" : [\"12 B\"],\n",
    "          \"CA\" : [\"50 M\"],\n",
    "          \"AU\" : [\"25 M\"]\n",
    "        } \n",
    "        \n",
    "table_b = { \"USA\" : [\"35 T\"],\n",
    "          \"UK\" : [\"3 T\"],\n",
    "          \"USA\" : [\"32 T\"],\n",
    "          \"AUS\" : [\"20 T\"],\n",
    "          \"CAL\" : [\"22 T\"]\n",
    "        } \n",
    "        \n",
    "matching = { \"US\" : \"USA\",\n",
    "          \"CN\" : \"\",\n",
    "          \"CA\" : \"CAL\",\n",
    "          \"AU\" : \"AUS\",\n",
    "        }\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "construct_graph(table_a,table_b).edges.data()\n",
    "bipartite_graph = construct_graph(table_a,table_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Detail possible edge cases for maximal bipartite matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. both tables have the same key-val pair like USA - 35 T\n",
    "# 2. table_a=(US,UK), table_b=(USA,US). natural matching would be US-USA, UK-US \n",
    "# but similarity metric makes the other matching just as likely?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Maximal Bipartite Matching Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('AU_300 M_1', 'AUS_20 T_2'),\n",
       " ('CAL_22 T_2', 'CA_300 M_1'),\n",
       " ('UK_3 T_2', 'CN_300 M_1'),\n",
       " ('USA_32 T_2', 'USA_35 T_1'),\n",
       " ('US_300 M_1', 'USA_35 T_2')}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.algorithms.matching.max_weight_matching(bipartite_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updates after meeting 03.25.2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize the code to \n",
    "\n",
    "1. allow for an arbitrary weight function and handle both maximum and minimum\n",
    "\n",
    "2. allow to be given two dataframes, and a function that gives you correspondence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'US': ('300 M', 1), 'CN': ('12 B', 2), 'CA': ('50 M', 3), 'AU': ('25 M', 4)}\n",
      "{'USA': ('32 T', 7), 'UK': ('3 T', 5), 'AUS': ('20 T', 8), 'CAL': ('22 T', 9)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EdgeDataView([(\"US_('300 M', 1)_1\", \"US_('300 M', 1)_2\", {'weight': -1.0}), (\"US_('300 M', 1)_1\", \"CN_('12 B', 2)_2\", {'weight': -0.3333333333333333}), (\"US_('300 M', 1)_1\", \"CA_('50 M', 3)_2\", {'weight': -0.3333333333333333}), (\"US_('300 M', 1)_1\", \"AU_('25 M', 4)_2\", {'weight': -0.3333333333333333}), (\"US_('300 M', 1)_2\", \"CN_('12 B', 2)_1\", {'weight': -0.3333333333333333}), (\"US_('300 M', 1)_2\", \"CA_('50 M', 3)_1\", {'weight': -0.3333333333333333}), (\"US_('300 M', 1)_2\", \"AU_('25 M', 4)_1\", {'weight': -0.3333333333333333}), (\"CN_('12 B', 2)_2\", \"CN_('12 B', 2)_1\", {'weight': -1.0}), (\"CN_('12 B', 2)_2\", \"CA_('50 M', 3)_1\", {'weight': -0.5}), (\"CN_('12 B', 2)_2\", \"AU_('25 M', 4)_1\", {'weight': -0.3333333333333333}), (\"CA_('50 M', 3)_2\", \"CN_('12 B', 2)_1\", {'weight': -0.5}), (\"CA_('50 M', 3)_2\", \"CA_('50 M', 3)_1\", {'weight': -1.0}), (\"CA_('50 M', 3)_2\", \"AU_('25 M', 4)_1\", {'weight': -0.3333333333333333}), (\"AU_('25 M', 4)_2\", \"CN_('12 B', 2)_1\", {'weight': -0.3333333333333333}), (\"AU_('25 M', 4)_2\", \"CA_('50 M', 3)_1\", {'weight': -0.3333333333333333}), (\"AU_('25 M', 4)_2\", \"AU_('25 M', 4)_1\", {'weight': -1.0})])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Transforms the given file to a pandas dataframe object if it was not one already\n",
    "Assumption: Assumes that the data starts from the 1st row of given file, does not use seperators such as \",\" or \";\"\n",
    "\n",
    "Input: Any file\n",
    "Output: A pandas dataframe object\n",
    "\"\"\"\n",
    "def convert_df(file):\n",
    "    if isinstance(file, pd.DataFrame):\n",
    "        return file\n",
    "    else:\n",
    "        df = pd.read_csv(file)\n",
    "        return df\n",
    "\"\"\"\n",
    "\n",
    "Calculates maximum weight for the matching\n",
    "\n",
    "Input: keys from 2 tables\n",
    "Output: weight for each matching to be used in the weight part of constructing the graph\n",
    "\"\"\"\n",
    "def calc_max_weight(key1, key2):\n",
    "    weight = 1/(1+similarity(key1,key2))\n",
    "    return weight\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Calculates minimum weight for the matching\n",
    "\n",
    "Input: keys from 2 tables\n",
    "Output: weight for each matching to be used in the weight part of constructing the graph\n",
    "\"\"\"\n",
    "def calc_min_weight(key1, key2):\n",
    "    weight = (-1)/(1+similarity(key1,key2))\n",
    "    return weight\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Converts the dataframe into dictionary for better accuracy matching of pairs. \n",
    "Assumption: The data has headers in the first row (description of what that column describes)\n",
    "\n",
    "Input: Any file\n",
    "Output: A dictionary in the form col1:col2 matching\n",
    "\"\"\"\n",
    "def make_dict(file):\n",
    "    V = list(file.to_dict('list').values())\n",
    "    keys = V[0]\n",
    "    values = zip(*V[1:])\n",
    "    table = dict(zip(keys,values))\n",
    "    return table\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "Constructs a maximal bipartite graph of the given two tables\n",
    "\n",
    "Input: Any 2 files in any format\n",
    "Output: A Bipartite Graph with Maximal Weights\n",
    "\"\"\"\n",
    "def updated_maximal_construct_graph(file_a, file_b):\n",
    "    table_a_unprocessed = convert_df(file_a)\n",
    "    table_b_unprocessed = convert_df(file_b)\n",
    "    bipartite_graph = nx.Graph()\n",
    "    \n",
    "    table_a = make_dict(table_a_unprocessed)\n",
    "    table_b = make_dict(table_b_unprocessed)\n",
    "    print(table_a)\n",
    "    print(table_b)\n",
    "    for key1, val1 in table_a.items():\n",
    "       # print(val1)\n",
    "        id1 = key1 + '_' + str(val1) + '_1'\n",
    "        for key2, val2 in table_b.items():\n",
    "            #add value to identifier to disitnguish two entries with different values\n",
    "            id2 = key2 + '_' + str(val2) + '_2' \n",
    "            bipartite_graph.add_edge(id1, id2, weight=calc_max_weight(key1, key2))\n",
    "            #edit distance and weight should be inv. prop.\n",
    "            #also adding 1 to denom. to prevent divide by 0\n",
    "            # add 1,2 to distinguish two key-value tuples belonging to different tables\n",
    "    return bipartite_graph\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Constructs a maximal bipartite graph of the given two tables\n",
    "\n",
    "Input: Any 2 files in any format\n",
    "Output: A Bipartite Graph with Minimal Weights\n",
    "\"\"\"\n",
    "def updated_minimal_construct_graph(file_a, file_b):\n",
    "    table_a_unprocessed = convert_df(file_a)\n",
    "    table_b_unprocessed = convert_df(file_a)\n",
    "    bipartite_graph = nx.Graph()\n",
    "    \n",
    "    table_a = make_dict(table_a_unprocessed)\n",
    "    table_b = make_dict(table_b_unprocessed)\n",
    "    for key1, val1 in table_a.items():\n",
    "        id1 = key1 + '_' + str(val1) + '_1'\n",
    "        for key2, val2 in table_b.items():\n",
    "            #add value to identifier to disitnguish two entries with different values\n",
    "            id2 = key2 + '_' + str(val2) + '_2' \n",
    "            bipartite_graph.add_edge(id1, id2, weight=calc_min_weight(key1, key2)) \n",
    "            #edit distance and weight should be inv. prop.\n",
    "            #also adding 1 to denom. to prevent divide by 0\n",
    "            # add 1,2 to distinguish two key-value tuples belonging to different tables\n",
    "    return bipartite_graph\n",
    "\n",
    "bipartite_graph_maximal = updated_maximal_construct_graph(\"table_a.csv\",\"table_b.csv\")\n",
    "#print(bipartite_graph_maximal.edges.data())\n",
    "bipartite_graph_minimal = updated_minimal_construct_graph(\"table_a.csv\", \"table_b.csv\")\n",
    "bipartite_graph_minimal.edges.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"CA_('50 M', 3)_1\": \"US_('300 M', 1)_2\", \"US_('300 M', 1)_1\": \"CN_('12 B', 2)_2\", \"AU_('25 M', 4)_1\": \"CA_('50 M', 3)_2\", \"CN_('12 B', 2)_1\": \"AU_('25 M', 4)_2\", \"AU_('25 M', 4)_2\": \"CN_('12 B', 2)_1\", \"CN_('12 B', 2)_2\": \"US_('300 M', 1)_1\", \"US_('300 M', 1)_2\": \"CA_('50 M', 3)_1\", \"CA_('50 M', 3)_2\": \"AU_('25 M', 4)_1\"}\n"
     ]
    }
   ],
   "source": [
    "#nx.algorithms.matching.max_weight_matching(bipartite_graph_maximal)\n",
    "print(nx.algorithms.bipartite.matching.maximum_matching(bipartite_graph_minimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, minimal matching gave a worse matching because our edit_distance function was configured to give a high number for words that were closer to each other. Therefore, maximal matching gave us better results than the minimal matching. However, this can be configured by changing the edit_distance function to make minimal matching give better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
