{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching on DBLP ACM Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset:\n",
    "* This data set was taken from the Benchmark datasets for entity resolution web page. \n",
    "* It contains bibliographic data, with 4 attributes: title, authors, venue, year. \n",
    "* There are 3 CSV files in this zip archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Measure how accurate the bipartite matching algorithm is using the datasets and the ground truth presented in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some assumptions and notes:\n",
    "* We are checking string similarity using the `titles` column of the 2 datasets\n",
    "\n",
    "* Added `encoded=latin-1` for pandas errors on file reading in the function `convert_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. We'll include the bipartite matching algorithm components from our previous findings. We can try out different string matching techniques. \n",
    "\n",
    "#### A good starting point would be to see how `editDistance` is working in this context, so we'll include the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import textdistance\n",
    "import editdistance\n",
    "\n",
    "def editDistance(str1, str2, m, n): \n",
    "  \n",
    "    # If first string is empty, the only option is to \n",
    "    # insert all characters of second string into first \n",
    "    if m == 0: \n",
    "         return n \n",
    "  \n",
    "    # If second string is empty, the only option is to \n",
    "    # remove all characters of first string \n",
    "    if n == 0: \n",
    "        return m \n",
    "  \n",
    "    # If last characters of two strings are same, nothing \n",
    "    # much to do. Ignore last characters and get count for \n",
    "    # remaining strings. \n",
    "    if str1[m-1]== str2[n-1]: \n",
    "        return editDistance(str1, str2, m-1, n-1) \n",
    "  \n",
    "    # If last characters are not same, consider all three \n",
    "    # operations on last character of first string, recursively \n",
    "    # compute minimum cost for all three operations and take \n",
    "    # minimum of three values. \n",
    "    return 1 + min(editDistance(str1, str2, m, n-1),    # Insert \n",
    "                   editDistance(str1, str2, m-1, n),    # Remove \n",
    "                   editDistance(str1, str2, m-1, n-1)    # Replace \n",
    "                   ) \n",
    "\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeDataView([('US_300 M_1', 'USA_32 T_2', {'weight': -0.25}), ('US_300 M_1', 'UK_3 T_2', {'weight': -0.25}), ('US_300 M_1', 'AUS_20 T_2', {'weight': -0.25}), ('US_300 M_1', 'CAL_22 T_2', {'weight': -0.2}), ('USA_32 T_2', 'CN_12 B_1', {'weight': -0.3333333333333333}), ('USA_32 T_2', 'CA_50 M_1', {'weight': -0.25}), ('USA_32 T_2', 'AU_25 M_1', {'weight': -0.25}), ('UK_3 T_2', 'CN_12 B_1', {'weight': -0.25}), ('UK_3 T_2', 'CA_50 M_1', {'weight': -0.25}), ('UK_3 T_2', 'AU_25 M_1', {'weight': -0.25}), ('AUS_20 T_2', 'CN_12 B_1', {'weight': -0.25}), ('AUS_20 T_2', 'CA_50 M_1', {'weight': -0.3333333333333333}), ('AUS_20 T_2', 'AU_25 M_1', {'weight': -0.3333333333333333}), ('CAL_22 T_2', 'CN_12 B_1', {'weight': -0.3333333333333333}), ('CAL_22 T_2', 'CA_50 M_1', {'weight': -0.25}), ('CAL_22 T_2', 'AU_25 M_1', {'weight': -0.3333333333333333})])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Transforms the given file to a pandas dataframe object if it was not one already\n",
    "Assumption: Assumes that the data starts from the 1st row of given file, does not use seperators such as \",\" or \";\"\n",
    "\n",
    "Input: 2 variables that will be measured for similarity\n",
    "Output: The desired similarity metric result\n",
    "\"\"\"\n",
    "def similarity_edit(x,y):\n",
    "    return editDistance(x,y,len(x),len(y))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Transforms the given file to a pandas dataframe object if it was not one already\n",
    "Assumption: Assumes that the data starts from the 1st row of given file, does not use seperators such as \",\" or \";\"\n",
    "\n",
    "Input: Any file\n",
    "Output: A pandas dataframe object\n",
    "\"\"\"\n",
    "def convert_df(file):\n",
    "    if isinstance(file, pd.DataFrame):\n",
    "        return file\n",
    "    else:\n",
    "        df = pd.read_csv(file, encoding='latin-1')\n",
    "        return df\n",
    "\"\"\"\n",
    "\n",
    "Calculates maximum weight for the matching\n",
    "\n",
    "Input: keys from 2 tables\n",
    "Output: weight for each matching to be used in the weight part of constructing the graph\n",
    "\"\"\"\n",
    "def calc_max_weight(key1, key2):\n",
    "    weight = textdistance.jaccard(key1,key2) #this library's implementation is slower than jaccard_similarity()\n",
    "    return weight\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Calculates minimum weight for the matching\n",
    "\n",
    "Input: keys from 2 tables\n",
    "Output: weight for each matching to be used in the weight part of constructing the graph\n",
    "\"\"\"\n",
    "def calc_min_weight(key1, key2):\n",
    "    weight = (-1)/(1+textdistance.jaccard(key1,key2)) #this library's implementation is slower than jaccard_similarity()\n",
    "    return weight\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Calculates maximum weight for the matching\n",
    "\n",
    "Input: keys from 2 tables\n",
    "Output: weight for each matching to be used in the weight part of constructing the graph\n",
    "\"\"\"\n",
    "def calc_max_weight_edit(key1, key2):\n",
    "    weight = (1)/(1+editdistance.eval(key1,key2))\n",
    "    return weight\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Calculates minimum weight for the matching\n",
    "\n",
    "Input: keys from 2 tables\n",
    "Output: weight for each matching to be used in the weight part of constructing the graph\n",
    "\"\"\"\n",
    "def calc_min_weight_edit(key1, key2):\n",
    "    weight = (-1)/(1+editdistance.eval(key1,key2))\n",
    "    return weight\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Converts the dataframe into dictionary for better accuracy matching of pairs. \n",
    "Assumption: The data has headers in the first row (description of what that column describes)\n",
    "\n",
    "Input: Any file\n",
    "Output: A dictionary in the form col1:col2 matching\n",
    "\"\"\"\n",
    "def make_dict(file):\n",
    "    V = list(file.to_dict('list').values())\n",
    "    keys = V[0]\n",
    "    values = zip(*V[1:])\n",
    "    table = dict(zip(keys,values))\n",
    "    return table\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Constructs a maximal bipartite graph of the given two tables\n",
    "\n",
    "Input: Any 2 files in any format\n",
    "Output: A Bipartite Graph with Maximal Weights\n",
    "\"\"\"\n",
    "def updated_maximal_construct_graph(file_a, file_b):\n",
    "    table_a_unprocessed = convert_df(file_a)\n",
    "    table_b_unprocessed = convert_df(file_b)\n",
    "    bipartite_graph = nx.Graph()\n",
    "    \n",
    "    table_a = make_dict(table_a_unprocessed)\n",
    "    table_b = make_dict(table_b_unprocessed)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    for key1, val1 in table_a.items():\n",
    "        comp_point_1 = val1[0]\n",
    "\n",
    "        id1 = str(key1) + '_' + str(comp_point_1) + '_1'\n",
    "        for key2, val2 in table_b.items():\n",
    "            comp_point_2 = val2[0]\n",
    "            i+=1\n",
    "            if i%100000 == 0:\n",
    "                print(str(round(100*i/len(table_a)/len(table_b),2))+'% complete')\n",
    "                \n",
    "            #add value to identifier to distinguish two entries with different values\n",
    "            \n",
    "            id2 = str(key2) + '_' + str(comp_point_2) + '_2' \n",
    "            bipartite_graph.add_edge(id1, id2, weight=calc_max_weight_edit(str(comp_point_1), str(comp_point_2)))\n",
    "            \n",
    "            #edit distance and weight should be inv. prop.\n",
    "            #also adding 1 to denom. to prevent divide by 0\n",
    "            # add 1,2 to distinguish two key-value tuples belonging to different tables\n",
    "            \n",
    "    return bipartite_graph\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Constructs a maximal bipartite graph of the given two tables\n",
    "\n",
    "Input: Any 2 files in any format\n",
    "Output: A Bipartite Graph with Minimal Weights\n",
    "\"\"\"\n",
    "def updated_minimal_construct_graph(file_a, file_b):\n",
    "    table_a_unprocessed = convert_df(file_a)\n",
    "    table_b_unprocessed = convert_df(file_b)\n",
    "    bipartite_graph = nx.Graph()\n",
    "    \n",
    "    table_a = make_dict(table_a_unprocessed)\n",
    "    table_b = make_dict(table_b_unprocessed)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    for key1, val1 in table_a.items():\n",
    "        comp_point_1 = val1[0]\n",
    "\n",
    "        id1 = str(key1) + '_' + str(comp_point_1) + '_1'\n",
    "        for key2, val2 in table_b.items():\n",
    "            comp_point_2 = val2[0]\n",
    "            i+=1\n",
    "            if i%100000 == 0:\n",
    "                print(str(round(100*i/len(table_a)/len(table_b),2))+'% complete')\n",
    "                \n",
    "            #add value to identifier to distinguish two entries with different values\n",
    "            \n",
    "            id2 = str(key2) + '_' + str(comp_point_2) + '_2' \n",
    "            bipartite_graph.add_edge(id1, id2, weight=calc_min_weight_edit(str(comp_point_1), str(comp_point_2)))\n",
    "            \n",
    "            #edit distance and weight should be inv. prop.\n",
    "            #also adding 1 to denom. to prevent divide by 0\n",
    "            # add 1,2 to distinguish two key-value tuples belonging to different tables\n",
    "            \n",
    "    return bipartite_graph\n",
    "\n",
    "bipartite_graph_maximal = updated_maximal_construct_graph(\"table_a.csv\",\"table_b.csv\")\n",
    "#print(bipartite_graph_maximal.edges.data())\n",
    "bipartite_graph_minimal = updated_minimal_construct_graph(\"table_a.csv\", \"table_b.csv\")\n",
    "bipartite_graph_minimal.edges.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CA_50 M_1': 'USA_32 T_2', 'US_300 M_1': 'UK_3 T_2', 'CN_12 B_1': 'AUS_20 T_2', 'AU_25 M_1': 'CAL_22 T_2', 'AUS_20 T_2': 'CN_12 B_1', 'USA_32 T_2': 'CA_50 M_1', 'UK_3 T_2': 'US_300 M_1', 'CAL_22 T_2': 'AU_25 M_1'}\n"
     ]
    }
   ],
   "source": [
    "#nx.algorithms.matching.max_weight_matching(bipartite_graph_maximal)\n",
    "print(nx.algorithms.bipartite.matching.maximum_matching(bipartite_graph_minimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the data for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sticking to the convention of table_a and table_b naming that we previously used for generalization purposes\n",
    "\n",
    "table_a = convert_df(\"ACM.csv\")\n",
    "\n",
    "table_b = convert_df(\"DBLP2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create a bipartite graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.67% complete\n",
      "3.33% complete\n",
      "5.0% complete\n",
      "6.67% complete\n",
      "8.33% complete\n",
      "10.0% complete\n",
      "11.66% complete\n",
      "13.33% complete\n",
      "15.0% complete\n",
      "16.66% complete\n",
      "18.33% complete\n",
      "20.0% complete\n",
      "21.66% complete\n",
      "23.33% complete\n",
      "25.0% complete\n",
      "26.66% complete\n",
      "28.33% complete\n",
      "29.99% complete\n",
      "31.66% complete\n",
      "33.33% complete\n",
      "34.99% complete\n",
      "36.66% complete\n",
      "38.33% complete\n",
      "39.99% complete\n",
      "41.66% complete\n",
      "43.33% complete\n",
      "44.99% complete\n",
      "46.66% complete\n",
      "48.32% complete\n",
      "49.99% complete\n",
      "51.66% complete\n",
      "53.32% complete\n",
      "54.99% complete\n",
      "56.66% complete\n",
      "58.32% complete\n",
      "59.99% complete\n",
      "61.66% complete\n",
      "63.32% complete\n",
      "64.99% complete\n",
      "66.65% complete\n",
      "68.32% complete\n",
      "69.99% complete\n",
      "71.65% complete\n",
      "73.32% complete\n",
      "74.99% complete\n",
      "76.65% complete\n",
      "78.32% complete\n",
      "79.99% complete\n",
      "81.65% complete\n",
      "83.32% complete\n",
      "84.98% complete\n",
      "86.65% complete\n",
      "88.32% complete\n",
      "89.98% complete\n",
      "91.65% complete\n",
      "93.32% complete\n",
      "94.98% complete\n",
      "96.65% complete\n",
      "98.32% complete\n",
      "99.98% complete\n",
      "---- Timing for Graph Construction ----\n",
      "67.205968 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6001104"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "graph_maximal = updated_maximal_construct_graph(table_a, table_b)\n",
    "timing_bg = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"---- Timing for Graph Construction ----\")\n",
    "print(timing_bg,\"seconds\")\n",
    "graph_maximal.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed, from the above output, there are 6 million edges. This creates a very heavy computation for the maximal matching algorithm. It is very likely that we will observe that the maximum matching algorithm will not halt given the very large number of edges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#large_comp = nx.algorithms.matching.max_weight_matching(graph_maximal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the max_weight_matching operation is O(n^3). The above operation does not halt given the large size of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update - 04.08.2020 Wednesday\n",
    "## Solution: Set a treshold to only connect the vertices that have the potential to be viable matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below solution sets a treshold of 0.3 for the jaccard similarity. However, this could be easily adjusted according to the similarity metric that we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Constructs a maximal bipartite graph of the given two tables according to the treshold similarity.\n",
    "The bipartite matching graph only includes those that have passed a certain similarity treshold.\n",
    "\n",
    "Input: Any 2 files in any format\n",
    "Output: A Bipartite Graph with Minimal Weights\n",
    "\"\"\"\n",
    "# convert to lowercase\n",
    "def treshold_updated_maximal_construct_graph(file_a, file_b, treshold_decimal):\n",
    "    table_a_unprocessed = convert_df(file_a)\n",
    "    table_b_unprocessed = convert_df(file_b)\n",
    "    bipartite_graph = nx.Graph()\n",
    "    \n",
    "    table_a = make_dict(table_a_unprocessed)\n",
    "    table_b = make_dict(table_b_unprocessed)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    for key1, val1 in table_a.items():\n",
    "        comp_point_1 = val1[0]\n",
    "      #  print(comp_point_1)\n",
    "        id1 = str(key1) + '_'+ str(comp_point_1) + '_1'\n",
    "\n",
    "        for key2, val2 in table_b.items():\n",
    "            comp_point_2 = val2[0]\n",
    "            dist = calc_max_weight_edit(str(comp_point_1).lower(),str(comp_point_2).lower())\n",
    "            i+=1\n",
    "            if i%100000 == 0:\n",
    "                print(str(round(100*i/len(table_a)/len(table_b),2))+'% complete')\n",
    "            if dist >= treshold_decimal:\n",
    "              #  print(key1,key2,dist)\n",
    "                #add value to identifier to disitnguish two entries with different values\n",
    "                id2 = str(key2) + '_' + str(comp_point_2) + '_2' \n",
    "                bipartite_graph.add_edge(id1, id2, weight=dist)\n",
    "                #edit distance and weight should be inv. prop.\n",
    "                #also adding 1 to denom. to prevent divide by 0\n",
    "                # add 1,2 to distinguish two key-value tuples belonging to different tables\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "    return bipartite_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.67% complete\n",
      "3.33% complete\n",
      "5.0% complete\n",
      "6.67% complete\n",
      "8.33% complete\n",
      "10.0% complete\n",
      "11.66% complete\n",
      "13.33% complete\n",
      "15.0% complete\n",
      "16.66% complete\n",
      "18.33% complete\n",
      "20.0% complete\n",
      "21.66% complete\n",
      "23.33% complete\n",
      "25.0% complete\n",
      "26.66% complete\n",
      "28.33% complete\n",
      "29.99% complete\n",
      "31.66% complete\n",
      "33.33% complete\n",
      "34.99% complete\n",
      "36.66% complete\n",
      "38.33% complete\n",
      "39.99% complete\n",
      "41.66% complete\n",
      "43.33% complete\n",
      "44.99% complete\n",
      "46.66% complete\n",
      "48.32% complete\n",
      "49.99% complete\n",
      "51.66% complete\n",
      "53.32% complete\n",
      "54.99% complete\n",
      "56.66% complete\n",
      "58.32% complete\n",
      "59.99% complete\n",
      "61.66% complete\n",
      "63.32% complete\n",
      "64.99% complete\n",
      "66.65% complete\n",
      "68.32% complete\n",
      "69.99% complete\n",
      "71.65% complete\n",
      "73.32% complete\n",
      "74.99% complete\n",
      "76.65% complete\n",
      "78.32% complete\n",
      "79.99% complete\n",
      "81.65% complete\n",
      "83.32% complete\n",
      "84.98% complete\n",
      "86.65% complete\n",
      "88.32% complete\n",
      "89.98% complete\n",
      "91.65% complete\n",
      "93.32% complete\n",
      "94.98% complete\n",
      "96.65% complete\n",
      "98.32% complete\n",
      "99.98% complete\n",
      "---- Timing for Graph Construction with Treshold Constraint ----\n",
      "46.010671 seconds\n",
      "---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\n",
      "5.301522 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2376"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "treshold_graph_maximal = treshold_updated_maximal_construct_graph(table_a, table_b, 0.3)\n",
    "timing_tresh = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"---- Timing for Graph Construction with Treshold Constraint ----\")\n",
    "print(timing_tresh,\"seconds\")\n",
    "#treshold_graph_maximal.number_of_edges()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Outputs the matching that has the maximal weight for each edge in the bipartite graph\n",
    "\n",
    "Input: A Bipartite Graph\n",
    "Output: A set of matchings. Ex: {('journals/vldb/MedjahedBBNE03__2', '775457__1')}\n",
    "\"\"\"\n",
    "now = datetime.datetime.now()\n",
    "matching_set = nx.algorithms.matching.max_weight_matching(treshold_graph_maximal)\n",
    "timing_match = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"---- Timing for Matching (Done on the graph constructed with the treshold constraint) ----\")\n",
    "print(timing_match,\"seconds\")\n",
    "treshold_graph_maximal.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As observed above, the graph construction with a treshold constraint was able to speed up the process by **27.745611 seconds**. I believe that this improvement in scalability will prove even better using even larger datasets.\n",
    "\n",
    "Most importantly, the matching algorithm halted now that we have less edges to do computations on. The matching algorithm averaged around a total of **5.26 seconds** (timing computation process: took an average result of a few runs of the above block) to do computations on **2376 edges**. The matching algorithm worked surprisingly faster now that it did computations on the edges that would actually matter and we skipped the edges that had a very low possibility of being similar. \n",
    "\n",
    "A question that I had in mind is whether the treshold that I have set (0.3) is able to approximately capture the number of matchings that are given in the perfect mapping. In order to investigate this, we can take a look at the perfect matching dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries in the perfect mapping file: 2224\n",
      "Difference between the entries in perfect mapping and the matching set: 141\n"
     ]
    }
   ],
   "source": [
    "perfect_mapping = convert_df(\"DBLP-ACM_perfectMapping.csv\")\n",
    "row = perfect_mapping.shape[0]\n",
    "print(\"Number of entries in the perfect mapping file:\", row)\n",
    "print(\"Difference between the entries in perfect mapping and the matching set:\", len(perfect_mapping) - len(matching_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, there definitely are less edges in the graph that I've constructed using the treshold. The perfect mapping file shows that there are **2224 rows** of matched data, whereas my algorithm is going to work on matching **2376 edges**, which only captures 65% of the matching already. It might be a good idea to lower the treshold to see if we can capture that extra **141 edges** that my functions could not capture so far, or go over the `treshold_updated_maximal_construct_graph` again to see if there are any mistakes in creating the graph according to a certain treshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.67% complete\n",
      "3.33% complete\n",
      "5.0% complete\n",
      "6.67% complete\n",
      "8.33% complete\n",
      "10.0% complete\n",
      "11.66% complete\n",
      "13.33% complete\n",
      "15.0% complete\n",
      "16.66% complete\n",
      "18.33% complete\n",
      "20.0% complete\n",
      "21.66% complete\n",
      "23.33% complete\n",
      "25.0% complete\n",
      "26.66% complete\n",
      "28.33% complete\n",
      "29.99% complete\n",
      "31.66% complete\n",
      "33.33% complete\n",
      "34.99% complete\n",
      "36.66% complete\n",
      "38.33% complete\n",
      "39.99% complete\n",
      "41.66% complete\n",
      "43.33% complete\n",
      "44.99% complete\n",
      "46.66% complete\n",
      "48.32% complete\n",
      "49.99% complete\n",
      "51.66% complete\n",
      "53.32% complete\n",
      "54.99% complete\n",
      "56.66% complete\n",
      "58.32% complete\n",
      "59.99% complete\n",
      "61.66% complete\n",
      "63.32% complete\n",
      "64.99% complete\n",
      "66.65% complete\n",
      "68.32% complete\n",
      "69.99% complete\n",
      "71.65% complete\n",
      "73.32% complete\n",
      "74.99% complete\n",
      "76.65% complete\n",
      "78.32% complete\n",
      "79.99% complete\n",
      "81.65% complete\n",
      "83.32% complete\n",
      "84.98% complete\n",
      "86.65% complete\n",
      "88.32% complete\n",
      "89.98% complete\n",
      "91.65% complete\n",
      "93.32% complete\n",
      "94.98% complete\n",
      "96.65% complete\n",
      "98.32% complete\n",
      "99.98% complete\n"
     ]
    }
   ],
   "source": [
    "lowered_tresh_graph = treshold_updated_maximal_construct_graph(table_a, table_b, 0.25)\n",
    "lowered_tresh_matching = nx.algorithms.matching.max_weight_matching(lowered_tresh_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of edges that the graph with a treshold of 0.25 contains is: 2384\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of edges that the graph with a treshold of 0.25 contains is:\", lowered_tresh_graph.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Constructs a maximal bipartite graph of the given two tables according to the treshold similarity.\n",
    "The bipartite matching graph only includes those that have passed a certain similarity treshold.\n",
    "\n",
    "Input: A set of vertices that have been matched according to the weight of the edges. Matching should be of type tuples inside a set.\n",
    "Output: A Bipartite Graph with Minimal Weights\n",
    "\"\"\"\n",
    "def retrieve(matching):\n",
    "    res2 = list(matching)\n",
    "    res_tuple = []\n",
    "    for i in res2:\n",
    "\n",
    "        if int(i[0].split(\"_\")[-1]) == 1:\n",
    "            idACM = i[0].split(\"_\")[0]\n",
    "            idDBLP = i[1].split(\"_\")[0]\n",
    "            res_tuple.append((idDBLP, idACM))\n",
    "            \n",
    "        if int(i[0].split(\"_\")[-1]) == 2:\n",
    "            idACM = i[1].split(\"_\")[0]\n",
    "            idDBLP = i[0].split(\"_\")[0]\n",
    "            res_tuple.append((idDBLP, idACM))\n",
    "            \n",
    "    return res_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Accuracy----\n",
      "{'false positive': 0.023043686989918433, 'false negative': 0.08539325842696632, 'accuracy': 0.9447539461467038}\n",
      "---- Timing ----\n",
      "0.0158 seconds\n",
      "----Accuracy----\n",
      "{'false positive': 0.02297750119674491, 'false negative': 0.0826966292134832, 'accuracy': 0.9462216040797403}\n",
      "---- Timing ----\n",
      "0.010587 seconds\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def eval_matching(matching):\n",
    "    f = open('DBLP-ACM_perfectMapping.csv', 'r', encoding = \"ISO-8859-1\")\n",
    "    reader = csv.reader(f, delimiter=',', quotechar='\"')\n",
    "    matches = set()\n",
    "    proposed_matches = set()\n",
    "\n",
    "    tp = set()\n",
    "    fp = set()\n",
    "    fn = set()\n",
    "    tn = set()\n",
    "\n",
    "    for row in reader:\n",
    "        matches.add((row[0],row[1]))\n",
    "\n",
    "    for m in matching:\n",
    "     #   print(m)\n",
    "        proposed_matches.add(m)\n",
    "\n",
    "        if m in matches:\n",
    "            tp.add(m)\n",
    "        else:\n",
    "            fp.add(m)\n",
    "\n",
    "    for m in matches:\n",
    "        if m not in proposed_matches:\n",
    "            fn.add(m)\n",
    "\n",
    "    prec = len(tp)/(len(tp) + len(fp))\n",
    "    rec = len(tp)/(len(tp) + len(fn))\n",
    "\n",
    "    return {'false positive': 1-prec, \n",
    "            'false negative': 1-rec,\n",
    "            'accuracy': 2*(prec*rec)/(prec+rec)}\n",
    "\n",
    "\n",
    "#prints out the accuracy for 0.3 threshold\n",
    "now = datetime.datetime.now()\n",
    "out = eval_matching(retrieve(matching_set)) # retrieve() returns a list of tuples of DBLP2 ids and ACM ids.\n",
    "timing = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"----Accuracy----\")\n",
    "print(out)\n",
    "print(\"---- Timing ----\")\n",
    "print(timing,\"seconds\")\n",
    "\n",
    "#prints out the accuracy for 0.25 threshold\n",
    "now = datetime.datetime.now()\n",
    "out = eval_matching(retrieve(lowered_tresh_matching)) # retrieve() returns a list of tuples of DBLP2 ids and ACM ids.\n",
    "timing_lower = (datetime.datetime.now()-now).total_seconds()\n",
    "print(\"----Accuracy----\")\n",
    "print(out)\n",
    "print(\"---- Timing ----\")\n",
    "print(timing_lower,\"seconds\")\n",
    "\n",
    "#prints out accuracy for 0 treshold ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying out a few different string similarity libraries (most promising ones being https://pypi.org/project/editdistance/0.3.1/ and https://pypi.org/project/textdistance/#description), I observed that the `jaccard_similarity` function that I wrote was surprisingly faster than the textdistance library implementation, so I've reverted the code to use `jaccard_similarity` instead again until we can find a better library for it. On the other hand, the `editDistance` library is a very fast implementation of the Levenshtein Distance in C. So I've written another function called `calc_max_weight_edit` and `calc_min_weight_edit` so that it is easier to switch between the similarity metrics in a more generalized code structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another observation worthy of noting is that the Jaccard and the Levenshtein similarity metrics gave the same accuracy rate (70%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigating the issue of $|edges_{bipartite_graph}| < |rows_{perfect_mapping}|$\n",
    "\n",
    "Coming back to the case of observing less edges than the rows that exist in the perfect mapping file, we can investigate this issue by checking which edges are absent according to the perfect mapping file and see if there is a pattern in these missing edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = []\n",
    "\"\"\"for (idDBLP, idACM in perfect_mapping:\n",
    "    if entry not in matching_set:\n",
    "        print(entry)\n",
    "        missing.append(entry)\n",
    "print(missing)\"\"\"\n",
    "\n",
    "mapped = set()\n",
    "for index, row in perfect_mapping.iterrows():\n",
    "    mapped.add((row[\"idDBLP\"], row[\"idACM\"]))\n",
    "    \n",
    "tp_inv = set()\n",
    "fp_inv = set()\n",
    "\n",
    "for i in matching_set:\n",
    "    if i in mapped:\n",
    "        tp_inv.add(i)\n",
    "    else:\n",
    "        fp_inv.add(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Resolved**:\n",
    "\n",
    "The val1 and val2 included tuples instead of just the titles. The tuples in values had all the other columns that weren't planned for in the similarity matching (matching would only be done on the title column in this instance). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea: Writing a generalized evaluation function for measuring whether the treshold can capture the number of matchings in perfect mapping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: Would we like to generalize the matching to \"the rest of the columns\" instead of using just one column? For example, such function would use a concatanated string of \"title, authors, venue, year\" instead of only using \"title\" to make the matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
