{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching on DBLP ACM Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset:\n",
    "* This data set was taken from the Benchmark datasets for entity resolution web page. \n",
    "* It contains bibliographic data, with 4 attributes: title, authors, venue, year. \n",
    "* There are 3 CSV files in this zip archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Measure how accurate the bipartite matching algorithm is using the datasets and the ground truth presented in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some assumptions and notes:\n",
    "* We are checking string similarity using the `titles` column of the 2 datasets\n",
    "\n",
    "* Added `encoded=latin-1` for pandas errors on file reading in the function `convert_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we'll include the bipartite matching algorithm components from our previous findings. We can try out different string matching techniques. \n",
    "\n",
    "#### A good starting point would be to see how `editDistance` is working in this context, so we'll include the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editDistance(str1, str2, m, n): \n",
    "  \n",
    "    # If first string is empty, the only option is to \n",
    "    # insert all characters of second string into first \n",
    "    if m == 0: \n",
    "         return n \n",
    "  \n",
    "    # If second string is empty, the only option is to \n",
    "    # remove all characters of first string \n",
    "    if n == 0: \n",
    "        return m \n",
    "  \n",
    "    # If last characters of two strings are same, nothing \n",
    "    # much to do. Ignore last characters and get count for \n",
    "    # remaining strings. \n",
    "    if str1[m-1]== str2[n-1]: \n",
    "        return editDistance(str1, str2, m-1, n-1) \n",
    "  \n",
    "    # If last characters are not same, consider all three \n",
    "    # operations on last character of first string, recursively \n",
    "    # compute minimum cost for all three operations and take \n",
    "    # minimum of three values. \n",
    "    return 1 + min(editDistance(str1, str2, m, n-1),    # Insert \n",
    "                   editDistance(str1, str2, m-1, n),    # Remove \n",
    "                   editDistance(str1, str2, m-1, n-1)    # Replace \n",
    "                   ) \n",
    "\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EdgeDataView([(\"US_('300 M', 1)_1\", \"US_('300 M', 1)_2\", {'weight': -0.5}), (\"US_('300 M', 1)_1\", \"CN_('12 B', 2)_2\", {'weight': -1.0}), (\"US_('300 M', 1)_1\", \"CA_('50 M', 3)_2\", {'weight': -1.0}), (\"US_('300 M', 1)_1\", \"AU_('25 M', 4)_2\", {'weight': -0.75}), (\"US_('300 M', 1)_2\", \"CN_('12 B', 2)_1\", {'weight': -1.0}), (\"US_('300 M', 1)_2\", \"CA_('50 M', 3)_1\", {'weight': -1.0}), (\"US_('300 M', 1)_2\", \"AU_('25 M', 4)_1\", {'weight': -0.75}), (\"CN_('12 B', 2)_2\", \"CN_('12 B', 2)_1\", {'weight': -0.5}), (\"CN_('12 B', 2)_2\", \"CA_('50 M', 3)_1\", {'weight': -0.75}), (\"CN_('12 B', 2)_2\", \"AU_('25 M', 4)_1\", {'weight': -1.0}), (\"CA_('50 M', 3)_2\", \"CN_('12 B', 2)_1\", {'weight': -0.75}), (\"CA_('50 M', 3)_2\", \"CA_('50 M', 3)_1\", {'weight': -0.5}), (\"CA_('50 M', 3)_2\", \"AU_('25 M', 4)_1\", {'weight': -0.75}), (\"AU_('25 M', 4)_2\", \"CN_('12 B', 2)_1\", {'weight': -1.0}), (\"AU_('25 M', 4)_2\", \"CA_('50 M', 3)_1\", {'weight': -0.75}), (\"AU_('25 M', 4)_2\", \"AU_('25 M', 4)_1\", {'weight': -0.5})])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Transforms the given file to a pandas dataframe object if it was not one already\n",
    "Assumption: Assumes that the data starts from the 1st row of given file, does not use seperators such as \",\" or \";\"\n",
    "\n",
    "Input: 2 variables that will be measured for similarity\n",
    "Output: The desired similarity metric result\n",
    "\"\"\"\n",
    "def similarity_edit(x,y):\n",
    "    return editDistance(x,y,len(x),len(y))\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Transforms the given file to a pandas dataframe object if it was not one already\n",
    "Assumption: Assumes that the data starts from the 1st row of given file, does not use seperators such as \",\" or \";\"\n",
    "\n",
    "Input: Any file\n",
    "Output: A pandas dataframe object\n",
    "\"\"\"\n",
    "def convert_df(file):\n",
    "    if isinstance(file, pd.DataFrame):\n",
    "        return file\n",
    "    else:\n",
    "        df = pd.read_csv(file, encoding='latin-1')\n",
    "        return df\n",
    "\"\"\"\n",
    "\n",
    "Calculates maximum weight for the matching\n",
    "\n",
    "Input: keys from 2 tables\n",
    "Output: weight for each matching to be used in the weight part of constructing the graph\n",
    "\"\"\"\n",
    "def calc_max_weight(key1, key2):\n",
    "    weight = jaccard_similarity(key1,key2)\n",
    "    return weight\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Calculates minimum weight for the matching\n",
    "\n",
    "Input: keys from 2 tables\n",
    "Output: weight for each matching to be used in the weight part of constructing the graph\n",
    "\"\"\"\n",
    "def calc_min_weight(key1, key2):\n",
    "    weight = (-1)/(1+jaccard_similarity(key1,key2))\n",
    "    return weight\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Converts the dataframe into dictionary for better accuracy matching of pairs. \n",
    "Assumption: The data has headers in the first row (description of what that column describes)\n",
    "\n",
    "Input: Any file\n",
    "Output: A dictionary in the form col1:col2 matching\n",
    "\"\"\"\n",
    "def make_dict(file):\n",
    "    V = list(file.to_dict('list').values())\n",
    "    keys = V[0]\n",
    "    values = zip(*V[1:])\n",
    "    table = dict(zip(keys,values))\n",
    "    return table\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "Constructs a maximal bipartite graph of the given two tables\n",
    "\n",
    "Input: Any 2 files in any format\n",
    "Output: A Bipartite Graph with Maximal Weights\n",
    "\"\"\"\n",
    "def updated_maximal_construct_graph(file_a, file_b):\n",
    "    table_a_unprocessed = convert_df(file_a)\n",
    "    table_b_unprocessed = convert_df(file_b)\n",
    "    bipartite_graph = nx.Graph()\n",
    "    \n",
    "    table_a = make_dict(table_a_unprocessed)\n",
    "    table_b = make_dict(table_b_unprocessed)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    for key1, val1 in table_a.items():\n",
    "       # print(val1)\n",
    "        id1 = str(key1) + '_' + str(val1) + '_1'\n",
    "        for key2, val2 in table_b.items():\n",
    "            i+=1\n",
    "            if i%100000 == 0:\n",
    "                print(str(round(100*i/len(table_a)/len(table_b),2))+'% complete')\n",
    "            #add value to identifier to disitnguish two entries with different values\n",
    "            id2 = str(key2) + '_' + str(val2) + '_2' \n",
    "            bipartite_graph.add_edge(id1, id2, weight=calc_max_weight(val1, val2))\n",
    "            #edit distance and weight should be inv. prop.\n",
    "            #also adding 1 to denom. to prevent divide by 0\n",
    "            # add 1,2 to distinguish two key-value tuples belonging to different tables\n",
    "    return bipartite_graph\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Constructs a maximal bipartite graph of the given two tables\n",
    "\n",
    "Input: Any 2 files in any format\n",
    "Output: A Bipartite Graph with Minimal Weights\n",
    "\"\"\"\n",
    "def updated_minimal_construct_graph(file_a, file_b):\n",
    "    table_a_unprocessed = convert_df(file_a)\n",
    "    table_b_unprocessed = convert_df(file_a)\n",
    "    bipartite_graph = nx.Graph()\n",
    "    \n",
    "    table_a = make_dict(table_a_unprocessed)\n",
    "    table_b = make_dict(table_b_unprocessed)\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    for key1, val1 in table_a.items():\n",
    "        id1 = str(key1) + '_' + str(val1) + '_1'\n",
    "        for key2, val2 in table_b.items():\n",
    "            i+=1\n",
    "            if i%100000 == 0:\n",
    "                print(str(round(100*i/len(table_a)/len(table_b),2))+'% complete')\n",
    "            #add value to identifier to distinguish two entries with different values\n",
    "            id2 = str(key2) + '_' + str(val2) + '_2' \n",
    "            bipartite_graph.add_edge(id1, id2, weight=calc_min_weight(key1, key2)) \n",
    "            #edit distance and weight should be inv. prop.\n",
    "            #also adding 1 to denom. to prevent divide by 0\n",
    "            # add 1,2 to distinguish two key-value tuples belonging to different tables\n",
    "    return bipartite_graph\n",
    "\n",
    "bipartite_graph_maximal = updated_maximal_construct_graph(\"table_a.csv\",\"table_b.csv\")\n",
    "#print(bipartite_graph_maximal.edges.data())\n",
    "bipartite_graph_minimal = updated_minimal_construct_graph(\"table_a.csv\", \"table_b.csv\")\n",
    "bipartite_graph_minimal.edges.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"US_('300 M', 1)_1\": \"US_('300 M', 1)_2\", \"CN_('12 B', 2)_1\": \"CN_('12 B', 2)_2\", \"AU_('25 M', 4)_1\": \"CA_('50 M', 3)_2\", \"CA_('50 M', 3)_1\": \"AU_('25 M', 4)_2\", \"CA_('50 M', 3)_2\": \"AU_('25 M', 4)_1\", \"US_('300 M', 1)_2\": \"US_('300 M', 1)_1\", \"CN_('12 B', 2)_2\": \"CN_('12 B', 2)_1\", \"AU_('25 M', 4)_2\": \"CA_('50 M', 3)_1\"}\n"
     ]
    }
   ],
   "source": [
    "#nx.algorithms.matching.max_weight_matching(bipartite_graph_maximal)\n",
    "print(nx.algorithms.bipartite.matching.maximum_matching(bipartite_graph_minimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondly, load the data for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sticking to the convention of table_a and table_b naming that we previously used for generalization purposes\n",
    "\n",
    "table_a = convert_df(\"ACM.csv\")\n",
    "\n",
    "table_b = convert_df(\"DBLP2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.67% complete\n",
      "3.33% complete\n",
      "5.0% complete\n",
      "6.67% complete\n",
      "8.33% complete\n",
      "10.0% complete\n",
      "11.66% complete\n",
      "13.33% complete\n",
      "15.0% complete\n",
      "16.66% complete\n",
      "18.33% complete\n",
      "20.0% complete\n",
      "21.66% complete\n",
      "23.33% complete\n",
      "25.0% complete\n",
      "26.66% complete\n",
      "28.33% complete\n",
      "29.99% complete\n",
      "31.66% complete\n",
      "33.33% complete\n",
      "34.99% complete\n",
      "36.66% complete\n",
      "38.33% complete\n",
      "39.99% complete\n",
      "41.66% complete\n",
      "43.33% complete\n",
      "44.99% complete\n",
      "46.66% complete\n",
      "48.32% complete\n",
      "49.99% complete\n",
      "51.66% complete\n",
      "53.32% complete\n",
      "54.99% complete\n",
      "56.66% complete\n",
      "58.32% complete\n",
      "59.99% complete\n",
      "61.66% complete\n",
      "63.32% complete\n",
      "64.99% complete\n",
      "66.65% complete\n",
      "68.32% complete\n",
      "69.99% complete\n",
      "71.65% complete\n",
      "73.32% complete\n",
      "74.99% complete\n",
      "76.65% complete\n",
      "78.32% complete\n",
      "79.99% complete\n",
      "81.65% complete\n",
      "83.32% complete\n",
      "84.98% complete\n",
      "86.65% complete\n",
      "88.32% complete\n",
      "89.98% complete\n",
      "91.65% complete\n",
      "93.32% complete\n",
      "94.98% complete\n",
      "96.65% complete\n",
      "98.32% complete\n",
      "99.98% complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6001104"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_maximal = updated_maximal_construct_graph(table_a, table_b)\n",
    "graph_maximal.number_of_edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(nx.algorithms.matching.max_weight_matching(graph_maximal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that the max_weight_matching operation is O(n^3). The above operation does not halt given the large size of the dataset. This might tell us that it is time that we look beyond the networkx package, because their maximal matching algorithms do not do any better than O(n^3). I've found the following alternatives:\n",
    "\n",
    "* https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html (this seems to be the best solution)\n",
    "\n",
    "* https://pypi.org/project/hungarian/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
