{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching on DBLP ACM Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset:\n",
    "* This data set was taken from the Benchmark datasets for entity resolution web page. \n",
    "* It contains bibliographic data, with 4 attributes: title, authors, venue, year. \n",
    "* There are 3 CSV files in this zip archive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Measure how accurate the bipartite matching algorithm is using the datasets and the ground truth presented in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some assumptions and notes:\n",
    "* We are checking string similarity using the `titles` column of the 2 datasets\n",
    "\n",
    "* Added `encoded=latin-1` for pandas errors on file reading in the function `convert_df`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First, we'll include the bipartite matching algorithm components from our previous findings. We can try out different string matching techniques. \n",
    "\n",
    "#### A good starting point would be to see how `editDistance` is working in this context, so we'll include the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def editDistance(str1, str2, m, n): \n",
    "  \n",
    "    # If first string is empty, the only option is to \n",
    "    # insert all characters of second string into first \n",
    "    if m == 0: \n",
    "         return n \n",
    "  \n",
    "    # If second string is empty, the only option is to \n",
    "    # remove all characters of first string \n",
    "    if n == 0: \n",
    "        return m \n",
    "  \n",
    "    # If last characters of two strings are same, nothing \n",
    "    # much to do. Ignore last characters and get count for \n",
    "    # remaining strings. \n",
    "    if str1[m-1]== str2[n-1]: \n",
    "        return editDistance(str1, str2, m-1, n-1) \n",
    "  \n",
    "    # If last characters are not same, consider all three \n",
    "    # operations on last character of first string, recursively \n",
    "    # compute minimum cost for all three operations and take \n",
    "    # minimum of three values. \n",
    "    return 1 + min(editDistance(str1, str2, m, n-1),    # Insert \n",
    "                   editDistance(str1, str2, m-1, n),    # Remove \n",
    "                   editDistance(str1, str2, m-1, n-1)    # Replace \n",
    "                   ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'US': ('300 M', 1), 'CN': ('12 B', 2), 'CA': ('50 M', 3), 'AU': ('25 M', 4)}\n",
      "{'USA': ('32 T', 7), 'UK': ('3 T', 5), 'AUS': ('20 T', 8), 'CAL': ('22 T', 9)}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EdgeDataView([(\"US_('300 M', 1)_1\", \"US_('300 M', 1)_2\", {'weight': -1.0}), (\"US_('300 M', 1)_1\", \"CN_('12 B', 2)_2\", {'weight': -0.3333333333333333}), (\"US_('300 M', 1)_1\", \"CA_('50 M', 3)_2\", {'weight': -0.3333333333333333}), (\"US_('300 M', 1)_1\", \"AU_('25 M', 4)_2\", {'weight': -0.3333333333333333}), (\"US_('300 M', 1)_2\", \"CN_('12 B', 2)_1\", {'weight': -0.3333333333333333}), (\"US_('300 M', 1)_2\", \"CA_('50 M', 3)_1\", {'weight': -0.3333333333333333}), (\"US_('300 M', 1)_2\", \"AU_('25 M', 4)_1\", {'weight': -0.3333333333333333}), (\"CN_('12 B', 2)_2\", \"CN_('12 B', 2)_1\", {'weight': -1.0}), (\"CN_('12 B', 2)_2\", \"CA_('50 M', 3)_1\", {'weight': -0.5}), (\"CN_('12 B', 2)_2\", \"AU_('25 M', 4)_1\", {'weight': -0.3333333333333333}), (\"CA_('50 M', 3)_2\", \"CN_('12 B', 2)_1\", {'weight': -0.5}), (\"CA_('50 M', 3)_2\", \"CA_('50 M', 3)_1\", {'weight': -1.0}), (\"CA_('50 M', 3)_2\", \"AU_('25 M', 4)_1\", {'weight': -0.3333333333333333}), (\"AU_('25 M', 4)_2\", \"CN_('12 B', 2)_1\", {'weight': -0.3333333333333333}), (\"AU_('25 M', 4)_2\", \"CA_('50 M', 3)_1\", {'weight': -0.3333333333333333}), (\"AU_('25 M', 4)_2\", \"AU_('25 M', 4)_1\", {'weight': -1.0})])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Transforms the given file to a pandas dataframe object if it was not one already\n",
    "Assumption: Assumes that the data starts from the 1st row of given file, does not use seperators such as \",\" or \";\"\n",
    "\n",
    "Input: Any file\n",
    "Output: A pandas dataframe object\n",
    "\"\"\"\n",
    "def convert_df(file):\n",
    "    if isinstance(file, pd.DataFrame):\n",
    "        return file\n",
    "    else:\n",
    "        df = pd.read_csv(file, encoding='latin-1')\n",
    "        return df\n",
    "\"\"\"\n",
    "\n",
    "Calculates maximum weight for the matching\n",
    "\n",
    "Input: keys from 2 tables\n",
    "Output: weight for each matching to be used in the weight part of constructing the graph\n",
    "\"\"\"\n",
    "def calc_max_weight(key1, key2):\n",
    "    weight = 1/(1+similarity(key1,key2))\n",
    "    return weight\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Calculates minimum weight for the matching\n",
    "\n",
    "Input: keys from 2 tables\n",
    "Output: weight for each matching to be used in the weight part of constructing the graph\n",
    "\"\"\"\n",
    "def calc_min_weight(key1, key2):\n",
    "    weight = (-1)/(1+similarity(key1,key2))\n",
    "    return weight\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Converts the dataframe into dictionary for better accuracy matching of pairs. \n",
    "Assumption: The data has headers in the first row (description of what that column describes)\n",
    "\n",
    "Input: Any file\n",
    "Output: A dictionary in the form col1:col2 matching\n",
    "\"\"\"\n",
    "def make_dict(file):\n",
    "    V = list(file.to_dict('list').values())\n",
    "    keys = V[0]\n",
    "    values = zip(*V[1:])\n",
    "    table = dict(zip(keys,values))\n",
    "    return table\n",
    "            \n",
    "\"\"\"\n",
    "\n",
    "Constructs a maximal bipartite graph of the given two tables\n",
    "\n",
    "Input: Any 2 files in any format\n",
    "Output: A Bipartite Graph with Maximal Weights\n",
    "\"\"\"\n",
    "def updated_maximal_construct_graph(file_a, file_b):\n",
    "    table_a_unprocessed = convert_df(file_a)\n",
    "    table_b_unprocessed = convert_df(file_b)\n",
    "    bipartite_graph = nx.Graph()\n",
    "    \n",
    "    table_a = make_dict(table_a_unprocessed)\n",
    "    table_b = make_dict(table_b_unprocessed)\n",
    "    print(table_a)\n",
    "    print(table_b)\n",
    "    for key1, val1 in table_a.items():\n",
    "       # print(val1)\n",
    "        id1 = key1 + '_' + str(val1) + '_1'\n",
    "        for key2, val2 in table_b.items():\n",
    "            #add value to identifier to disitnguish two entries with different values\n",
    "            id2 = key2 + '_' + str(val2) + '_2' \n",
    "            bipartite_graph.add_edge(id1, id2, weight=calc_max_weight(key1, key2))\n",
    "            #edit distance and weight should be inv. prop.\n",
    "            #also adding 1 to denom. to prevent divide by 0\n",
    "            # add 1,2 to distinguish two key-value tuples belonging to different tables\n",
    "    return bipartite_graph\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Constructs a maximal bipartite graph of the given two tables\n",
    "\n",
    "Input: Any 2 files in any format\n",
    "Output: A Bipartite Graph with Minimal Weights\n",
    "\"\"\"\n",
    "def updated_minimal_construct_graph(file_a, file_b):\n",
    "    table_a_unprocessed = convert_df(file_a)\n",
    "    table_b_unprocessed = convert_df(file_a)\n",
    "    bipartite_graph = nx.Graph()\n",
    "    \n",
    "    table_a = make_dict(table_a_unprocessed)\n",
    "    table_b = make_dict(table_b_unprocessed)\n",
    "    for key1, val1 in table_a.items():\n",
    "        id1 = key1 + '_' + str(val1) + '_1'\n",
    "        for key2, val2 in table_b.items():\n",
    "            #add value to identifier to disitnguish two entries with different values\n",
    "            id2 = key2 + '_' + str(val2) + '_2' \n",
    "            bipartite_graph.add_edge(id1, id2, weight=calc_min_weight(key1, key2)) \n",
    "            #edit distance and weight should be inv. prop.\n",
    "            #also adding 1 to denom. to prevent divide by 0\n",
    "            # add 1,2 to distinguish two key-value tuples belonging to different tables\n",
    "    return bipartite_graph\n",
    "\n",
    "bipartite_graph_maximal = updated_maximal_construct_graph(\"table_a.csv\",\"table_b.csv\")\n",
    "#print(bipartite_graph_maximal.edges.data())\n",
    "bipartite_graph_minimal = updated_minimal_construct_graph(\"table_a.csv\", \"table_b.csv\")\n",
    "bipartite_graph_minimal.edges.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"CN_('12 B', 2)_1\": \"US_('300 M', 1)_2\", \"US_('300 M', 1)_1\": \"CN_('12 B', 2)_2\", \"AU_('25 M', 4)_1\": \"CA_('50 M', 3)_2\", \"CA_('50 M', 3)_1\": \"AU_('25 M', 4)_2\", \"US_('300 M', 1)_2\": \"CN_('12 B', 2)_1\", \"AU_('25 M', 4)_2\": \"CA_('50 M', 3)_1\", \"CA_('50 M', 3)_2\": \"AU_('25 M', 4)_1\", \"CN_('12 B', 2)_2\": \"US_('300 M', 1)_1\"}\n"
     ]
    }
   ],
   "source": [
    "#nx.algorithms.matching.max_weight_matching(bipartite_graph_maximal)\n",
    "print(nx.algorithms.bipartite.matching.maximum_matching(bipartite_graph_minimal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondly, load the data for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sticking to the convention of table_a and table_b naming that we previously used for generalization purposes\n",
    "\n",
    "table_a = convert_df(\"ACM.csv\")\n",
    "\n",
    "table_b = convert_df(\"DBLP2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
